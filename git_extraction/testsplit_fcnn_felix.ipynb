{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the CSV file into a DataFrame.\n",
    "2. Parse the \"Radiomics\" column, as it contains JSON data.\n",
    "3. Remove columns with the same values across all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(309, 103)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# create random seed for reproducibility\n",
    "ran_seed = 42\n",
    "\n",
    "# Load the data from DF_Radiomics_noduls_with_diagnose.csv\n",
    "file_path = \"DF_Radiomics_noduls_with_diagnose.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Convert the 'Labels' column to an integer\n",
    "data['Labels'] = data['Labels'].astype(int)\n",
    "\n",
    "# drop all rows where the label == 0\n",
    "data = data[data.Labels != 0]\n",
    "\n",
    "# Parse the JSON in the 'Radiomics' column\n",
    "data['Radiomics'] = data['Radiomics'].apply(json.loads)\n",
    "\n",
    "# Convert the 'Radiomics' column into separate columns\n",
    "radiomics_data = pd.json_normalize(data['Radiomics'])\n",
    "\n",
    "\n",
    "# Drop the original 'Radiomics' column\n",
    "data = data.drop('Radiomics', axis=1)\n",
    "\n",
    "\n",
    "# Reset the indices of both DataFrames\n",
    "data = data.reset_index(drop=True)\n",
    "radiomics_data = radiomics_data.reset_index(drop=True)\n",
    "\n",
    "# Combine the data with the new radiomics columns\n",
    "data = pd.concat([data, radiomics_data], axis=1)\n",
    "\n",
    "# Remove columns with the same value across all rows\n",
    "data = data.loc[:, (data != data.iloc[0]).any()]\n",
    "\n",
    "#remove columns with all NaN values\n",
    "data = data.dropna(axis=1, how='all')\n",
    "\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Node</th>\n",
       "      <th>Labels</th>\n",
       "      <th>diagnostics_Image-original_Hash</th>\n",
       "      <th>diagnostics_Image-original_Spacing</th>\n",
       "      <th>diagnostics_Image-original_Size</th>\n",
       "      <th>diagnostics_Image-original_Mean</th>\n",
       "      <th>diagnostics_Image-original_Minimum</th>\n",
       "      <th>diagnostics_Image-original_Maximum</th>\n",
       "      <th>diagnostics_Mask-original_Hash</th>\n",
       "      <th>...</th>\n",
       "      <th>original_gldm_GrayLevelNonUniformity</th>\n",
       "      <th>original_gldm_GrayLevelVariance</th>\n",
       "      <th>original_gldm_HighGrayLevelEmphasis</th>\n",
       "      <th>original_gldm_LargeDependenceEmphasis</th>\n",
       "      <th>original_gldm_LargeDependenceHighGrayLevelEmphasis</th>\n",
       "      <th>original_gldm_LargeDependenceLowGrayLevelEmphasis</th>\n",
       "      <th>original_gldm_LowGrayLevelEmphasis</th>\n",
       "      <th>original_gldm_SmallDependenceEmphasis</th>\n",
       "      <th>original_gldm_SmallDependenceHighGrayLevelEmphasis</th>\n",
       "      <th>original_gldm_SmallDependenceLowGrayLevelEmphasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIDC-IDRI-0068</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>3</td>\n",
       "      <td>bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28</td>\n",
       "      <td>[0.683594, 0.683594, 1.25]</td>\n",
       "      <td>[512, 512, 261]</td>\n",
       "      <td>-1026.065264</td>\n",
       "      <td>-3024.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>0506d1d0d6522eddd1640c8ea75c2fc5a9266270</td>\n",
       "      <td>...</td>\n",
       "      <td>7.355556</td>\n",
       "      <td>60.706173</td>\n",
       "      <td>469.644444</td>\n",
       "      <td>23.444444</td>\n",
       "      <td>16578.377778</td>\n",
       "      <td>0.053875</td>\n",
       "      <td>0.021012</td>\n",
       "      <td>0.488461</td>\n",
       "      <td>152.929922</td>\n",
       "      <td>0.019809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIDC-IDRI-0068</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>3</td>\n",
       "      <td>bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28</td>\n",
       "      <td>[0.683594, 0.683594, 1.25]</td>\n",
       "      <td>[512, 512, 261]</td>\n",
       "      <td>-1026.065264</td>\n",
       "      <td>-3024.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>9d7da356d43e2f7ad7f374f6c193e97f6088d7c7</td>\n",
       "      <td>...</td>\n",
       "      <td>7.467153</td>\n",
       "      <td>72.801002</td>\n",
       "      <td>471.051095</td>\n",
       "      <td>17.496350</td>\n",
       "      <td>13573.328467</td>\n",
       "      <td>0.110650</td>\n",
       "      <td>0.024328</td>\n",
       "      <td>0.494688</td>\n",
       "      <td>165.356306</td>\n",
       "      <td>0.010062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LIDC-IDRI-0068</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>3</td>\n",
       "      <td>bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28</td>\n",
       "      <td>[0.683594, 0.683594, 1.25]</td>\n",
       "      <td>[512, 512, 261]</td>\n",
       "      <td>-1026.065264</td>\n",
       "      <td>-3024.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>c0a43747a23d26b107e21614525f2fd8870ffefc</td>\n",
       "      <td>...</td>\n",
       "      <td>7.685185</td>\n",
       "      <td>43.527006</td>\n",
       "      <td>277.787037</td>\n",
       "      <td>20.370370</td>\n",
       "      <td>9310.490741</td>\n",
       "      <td>0.084481</td>\n",
       "      <td>0.031811</td>\n",
       "      <td>0.463956</td>\n",
       "      <td>84.174037</td>\n",
       "      <td>0.027819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIDC-IDRI-0068</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>3</td>\n",
       "      <td>bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28</td>\n",
       "      <td>[0.683594, 0.683594, 1.25]</td>\n",
       "      <td>[512, 512, 261]</td>\n",
       "      <td>-1026.065264</td>\n",
       "      <td>-3024.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>72a09dc3f5d5d146b13402b8ef109422cc3f38a5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.780220</td>\n",
       "      <td>35.367709</td>\n",
       "      <td>229.219780</td>\n",
       "      <td>18.780220</td>\n",
       "      <td>7065.923077</td>\n",
       "      <td>0.084783</td>\n",
       "      <td>0.026368</td>\n",
       "      <td>0.465301</td>\n",
       "      <td>67.725183</td>\n",
       "      <td>0.021973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIDC-IDRI-0072</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>1</td>\n",
       "      <td>54705f26f9320581c90452445aa820fe9630d5e9</td>\n",
       "      <td>[0.732422, 0.732422, 1.25]</td>\n",
       "      <td>[512, 512, 305]</td>\n",
       "      <td>-871.936330</td>\n",
       "      <td>-3024.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>05efcefff38c73903c3d7839bb987a49176f6068</td>\n",
       "      <td>...</td>\n",
       "      <td>629.334146</td>\n",
       "      <td>45.147393</td>\n",
       "      <td>1253.131545</td>\n",
       "      <td>28.918031</td>\n",
       "      <td>43475.541623</td>\n",
       "      <td>0.020967</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.262518</td>\n",
       "      <td>254.476429</td>\n",
       "      <td>0.000632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Patient     Node  Labels           diagnostics_Image-original_Hash  \\\n",
       "0  LIDC-IDRI-0068  Node_N1       3  bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28   \n",
       "1  LIDC-IDRI-0068  Node_N1       3  bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28   \n",
       "2  LIDC-IDRI-0068  Node_N1       3  bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28   \n",
       "3  LIDC-IDRI-0068  Node_N1       3  bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28   \n",
       "4  LIDC-IDRI-0072  Node_N1       1  54705f26f9320581c90452445aa820fe9630d5e9   \n",
       "\n",
       "  diagnostics_Image-original_Spacing diagnostics_Image-original_Size  \\\n",
       "0         [0.683594, 0.683594, 1.25]                 [512, 512, 261]   \n",
       "1         [0.683594, 0.683594, 1.25]                 [512, 512, 261]   \n",
       "2         [0.683594, 0.683594, 1.25]                 [512, 512, 261]   \n",
       "3         [0.683594, 0.683594, 1.25]                 [512, 512, 261]   \n",
       "4         [0.732422, 0.732422, 1.25]                 [512, 512, 305]   \n",
       "\n",
       "   diagnostics_Image-original_Mean  diagnostics_Image-original_Minimum  \\\n",
       "0                     -1026.065264                             -3024.0   \n",
       "1                     -1026.065264                             -3024.0   \n",
       "2                     -1026.065264                             -3024.0   \n",
       "3                     -1026.065264                             -3024.0   \n",
       "4                      -871.936330                             -3024.0   \n",
       "\n",
       "   diagnostics_Image-original_Maximum  \\\n",
       "0                              3071.0   \n",
       "1                              3071.0   \n",
       "2                              3071.0   \n",
       "3                              3071.0   \n",
       "4                              3071.0   \n",
       "\n",
       "             diagnostics_Mask-original_Hash  ...  \\\n",
       "0  0506d1d0d6522eddd1640c8ea75c2fc5a9266270  ...   \n",
       "1  9d7da356d43e2f7ad7f374f6c193e97f6088d7c7  ...   \n",
       "2  c0a43747a23d26b107e21614525f2fd8870ffefc  ...   \n",
       "3  72a09dc3f5d5d146b13402b8ef109422cc3f38a5  ...   \n",
       "4  05efcefff38c73903c3d7839bb987a49176f6068  ...   \n",
       "\n",
       "  original_gldm_GrayLevelNonUniformity original_gldm_GrayLevelVariance  \\\n",
       "0                             7.355556                       60.706173   \n",
       "1                             7.467153                       72.801002   \n",
       "2                             7.685185                       43.527006   \n",
       "3                             6.780220                       35.367709   \n",
       "4                           629.334146                       45.147393   \n",
       "\n",
       "  original_gldm_HighGrayLevelEmphasis  original_gldm_LargeDependenceEmphasis  \\\n",
       "0                          469.644444                              23.444444   \n",
       "1                          471.051095                              17.496350   \n",
       "2                          277.787037                              20.370370   \n",
       "3                          229.219780                              18.780220   \n",
       "4                         1253.131545                              28.918031   \n",
       "\n",
       "   original_gldm_LargeDependenceHighGrayLevelEmphasis  \\\n",
       "0                                       16578.377778    \n",
       "1                                       13573.328467    \n",
       "2                                        9310.490741    \n",
       "3                                        7065.923077    \n",
       "4                                       43475.541623    \n",
       "\n",
       "  original_gldm_LargeDependenceLowGrayLevelEmphasis  \\\n",
       "0                                          0.053875   \n",
       "1                                          0.110650   \n",
       "2                                          0.084481   \n",
       "3                                          0.084783   \n",
       "4                                          0.020967   \n",
       "\n",
       "  original_gldm_LowGrayLevelEmphasis  original_gldm_SmallDependenceEmphasis  \\\n",
       "0                           0.021012                               0.488461   \n",
       "1                           0.024328                               0.494688   \n",
       "2                           0.031811                               0.463956   \n",
       "3                           0.026368                               0.465301   \n",
       "4                           0.001319                               0.262518   \n",
       "\n",
       "   original_gldm_SmallDependenceHighGrayLevelEmphasis  \\\n",
       "0                                         152.929922    \n",
       "1                                         165.356306    \n",
       "2                                          84.174037    \n",
       "3                                          67.725183    \n",
       "4                                         254.476429    \n",
       "\n",
       "   original_gldm_SmallDependenceLowGrayLevelEmphasis  \n",
       "0                                           0.019809  \n",
       "1                                           0.010062  \n",
       "2                                           0.027819  \n",
       "3                                           0.021973  \n",
       "4                                           0.000632  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove hash columns\n",
    "data = data.drop(['diagnostics_Image-original_Hash', 'diagnostics_Mask-original_Hash'], axis=1)\n",
    "\n",
    "# ok looks like all the objeckt columns except of \"Patient\" & \"Node\" are in this form [0.683594, 0.683594, 1.25] which is a list of multiple floats\n",
    "# exploade them into multiple columns\n",
    "\n",
    "object_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove 'Patient' and 'Node' from the list\n",
    "object_columns.remove('Patient')\n",
    "object_columns.remove('Node')\n",
    "\n",
    "# Explode the lists in each object column into multiple columns\n",
    "for column in object_columns:\n",
    "    # Convert each list to a Series and expand it into multiple columns\n",
    "    expanded_columns = data[column].apply(pd.Series)\n",
    "    \n",
    "    # Rename the expanded columns to have the original column name as a prefix\n",
    "    expanded_columns = expanded_columns.rename(columns=lambda x: f\"{column}_{x}\")\n",
    "    \n",
    "    # Drop the original column from the DataFrame\n",
    "    data = data.drop(column, axis=1)\n",
    "    \n",
    "    # Concatenate the expanded columns to the DataFrame\n",
    "    data = pd.concat([data, expanded_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist, skipping this step\n"
     ]
    }
   ],
   "source": [
    "# Create a stratified split\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['Labels'], random_state=ran_seed)\n",
    "\n",
    "# if the files already exist, skip this step\n",
    "if os.path.isfile('DF_Radiomics_noduls_with_diagnose_train_data.csv') and os.path.isfile('DF_Radiomics_noduls_with_diagnose_test_data.csv'):\n",
    "    print(\"Files already exist, skipping this step\")\n",
    "else:\n",
    "    # Save the data to CSV files\n",
    "    train_data.to_csv('DF_Radiomics_noduls_with_diagnose_train_data.csv', index=False)\n",
    "    test_data.to_csv('DF_Radiomics_noduls_with_diagnose_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (247, 118)\n",
      "Test data: (62, 118)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data:\", train_data.shape)\n",
    "print(\"Test data:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Node</th>\n",
       "      <th>Labels</th>\n",
       "      <th>diagnostics_Image-original_Mean</th>\n",
       "      <th>diagnostics_Image-original_Minimum</th>\n",
       "      <th>diagnostics_Image-original_Maximum</th>\n",
       "      <th>diagnostics_Mask-original_VoxelNum</th>\n",
       "      <th>diagnostics_Mask-original_VolumeNum</th>\n",
       "      <th>original_firstorder_10Percentile</th>\n",
       "      <th>original_firstorder_90Percentile</th>\n",
       "      <th>...</th>\n",
       "      <th>diagnostics_Mask-original_BoundingBox_2</th>\n",
       "      <th>diagnostics_Mask-original_BoundingBox_3</th>\n",
       "      <th>diagnostics_Mask-original_BoundingBox_4</th>\n",
       "      <th>diagnostics_Mask-original_BoundingBox_5</th>\n",
       "      <th>diagnostics_Mask-original_CenterOfMassIndex_0</th>\n",
       "      <th>diagnostics_Mask-original_CenterOfMassIndex_1</th>\n",
       "      <th>diagnostics_Mask-original_CenterOfMassIndex_2</th>\n",
       "      <th>diagnostics_Mask-original_CenterOfMass_0</th>\n",
       "      <th>diagnostics_Mask-original_CenterOfMass_1</th>\n",
       "      <th>diagnostics_Mask-original_CenterOfMass_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LIDC-IDRI-0137</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>3</td>\n",
       "      <td>-671.885608</td>\n",
       "      <td>-2048.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>175.5</td>\n",
       "      <td>850.5</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>332.692308</td>\n",
       "      <td>389.538462</td>\n",
       "      <td>30.307692</td>\n",
       "      <td>53.215868</td>\n",
       "      <td>83.626926</td>\n",
       "      <td>-321.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>LIDC-IDRI-0377</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>2</td>\n",
       "      <td>-882.321409</td>\n",
       "      <td>-3024.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>2402</td>\n",
       "      <td>1</td>\n",
       "      <td>-307.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>169</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>382.402998</td>\n",
       "      <td>308.854288</td>\n",
       "      <td>173.039550</td>\n",
       "      <td>92.739302</td>\n",
       "      <td>28.898399</td>\n",
       "      <td>-68.460564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>LIDC-IDRI-0167</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>1</td>\n",
       "      <td>-664.766231</td>\n",
       "      <td>-2048.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>-444.5</td>\n",
       "      <td>-66.5</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>70.267857</td>\n",
       "      <td>174.964286</td>\n",
       "      <td>50.321429</td>\n",
       "      <td>-136.237780</td>\n",
       "      <td>-53.812866</td>\n",
       "      <td>-234.696429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>LIDC-IDRI-0272</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>3</td>\n",
       "      <td>-824.358062</td>\n",
       "      <td>-2048.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>-447.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>209.313725</td>\n",
       "      <td>390.941176</td>\n",
       "      <td>81.568627</td>\n",
       "      <td>-47.673652</td>\n",
       "      <td>80.722794</td>\n",
       "      <td>-109.078431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>LIDC-IDRI-0234</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>1</td>\n",
       "      <td>-708.012378</td>\n",
       "      <td>-2048.0</td>\n",
       "      <td>3029.0</td>\n",
       "      <td>251</td>\n",
       "      <td>1</td>\n",
       "      <td>-569.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>367.756972</td>\n",
       "      <td>310.848606</td>\n",
       "      <td>41.689243</td>\n",
       "      <td>65.179121</td>\n",
       "      <td>43.765426</td>\n",
       "      <td>-236.276892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Patient     Node  Labels  diagnostics_Image-original_Mean  \\\n",
       "23   LIDC-IDRI-0137  Node_N1       3                      -671.885608   \n",
       "263  LIDC-IDRI-0377  Node_N1       2                      -882.321409   \n",
       "44   LIDC-IDRI-0167  Node_N1       1                      -664.766231   \n",
       "219  LIDC-IDRI-0272  Node_N1       3                      -824.358062   \n",
       "143  LIDC-IDRI-0234  Node_N1       1                      -708.012378   \n",
       "\n",
       "     diagnostics_Image-original_Minimum  diagnostics_Image-original_Maximum  \\\n",
       "23                              -2048.0                              3071.0   \n",
       "263                             -3024.0                              3071.0   \n",
       "44                              -2048.0                              3071.0   \n",
       "219                             -2048.0                              3071.0   \n",
       "143                             -2048.0                              3029.0   \n",
       "\n",
       "     diagnostics_Mask-original_VoxelNum  diagnostics_Mask-original_VolumeNum  \\\n",
       "23                                   26                                    1   \n",
       "263                                2402                                    1   \n",
       "44                                   56                                    1   \n",
       "219                                  51                                    1   \n",
       "143                                 251                                    1   \n",
       "\n",
       "     original_firstorder_10Percentile  original_firstorder_90Percentile  ...  \\\n",
       "23                              175.5                             850.5  ...   \n",
       "263                            -307.0                              61.0  ...   \n",
       "44                             -444.5                             -66.5  ...   \n",
       "219                            -447.0                             102.0  ...   \n",
       "143                            -569.0                              82.0  ...   \n",
       "\n",
       "     diagnostics_Mask-original_BoundingBox_2  \\\n",
       "23                                        30   \n",
       "263                                      169   \n",
       "44                                        50   \n",
       "219                                       81   \n",
       "143                                       41   \n",
       "\n",
       "     diagnostics_Mask-original_BoundingBox_3  \\\n",
       "23                                         4   \n",
       "263                                       29   \n",
       "44                                         6   \n",
       "219                                        6   \n",
       "143                                       11   \n",
       "\n",
       "     diagnostics_Mask-original_BoundingBox_4  \\\n",
       "23                                         6   \n",
       "263                                       24   \n",
       "44                                         9   \n",
       "219                                        7   \n",
       "143                                       14   \n",
       "\n",
       "     diagnostics_Mask-original_BoundingBox_5  \\\n",
       "23                                         2   \n",
       "263                                        9   \n",
       "44                                         2   \n",
       "219                                        2   \n",
       "143                                        3   \n",
       "\n",
       "     diagnostics_Mask-original_CenterOfMassIndex_0  \\\n",
       "23                                      332.692308   \n",
       "263                                     382.402998   \n",
       "44                                       70.267857   \n",
       "219                                     209.313725   \n",
       "143                                     367.756972   \n",
       "\n",
       "     diagnostics_Mask-original_CenterOfMassIndex_1  \\\n",
       "23                                      389.538462   \n",
       "263                                     308.854288   \n",
       "44                                      174.964286   \n",
       "219                                     390.941176   \n",
       "143                                     310.848606   \n",
       "\n",
       "     diagnostics_Mask-original_CenterOfMassIndex_2  \\\n",
       "23                                       30.307692   \n",
       "263                                     173.039550   \n",
       "44                                       50.321429   \n",
       "219                                      81.568627   \n",
       "143                                      41.689243   \n",
       "\n",
       "     diagnostics_Mask-original_CenterOfMass_0  \\\n",
       "23                                  53.215868   \n",
       "263                                 92.739302   \n",
       "44                                -136.237780   \n",
       "219                                -47.673652   \n",
       "143                                 65.179121   \n",
       "\n",
       "     diagnostics_Mask-original_CenterOfMass_1  \\\n",
       "23                                  83.626926   \n",
       "263                                 28.898399   \n",
       "44                                 -53.812866   \n",
       "219                                 80.722794   \n",
       "143                                 43.765426   \n",
       "\n",
       "     diagnostics_Mask-original_CenterOfMass_2  \n",
       "23                                -321.730769  \n",
       "263                                -68.460564  \n",
       "44                                -234.696429  \n",
       "219                               -109.078431  \n",
       "143                               -236.276892  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data already exists\n"
     ]
    }
   ],
   "source": [
    "# if DF_Radiomics_noduls_with_diagnose_train_data_scaled.csv and DF_Radiomics_noduls_with_diagnose_test_data_scaled.csv already exist, skip this step\n",
    "# otherwise scale the data and save it to CSV files\n",
    "if os.path.isfile('DF_Radiomics_noduls_with_diagnose_train_data_scaled.csv') and os.path.isfile('DF_Radiomics_noduls_with_diagnose_test_data_scaled.csv'):\n",
    "    print(\"Scaled data already exists\")\n",
    "else:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Get all column names\n",
    "    all_columns = train_data.columns.tolist()\n",
    "\n",
    "    # Exclude the first three columns\n",
    "    features = all_columns[3:]\n",
    "\n",
    "    # Create a stratified split\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['Labels'])\n",
    "\n",
    "    # Create a scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training data and transform both training and test data\n",
    "    train_data[features] = scaler.fit_transform(train_data[features])\n",
    "    test_data[features] = scaler.transform(test_data[features])\n",
    "\n",
    "    # Save the data to CSV files\n",
    "    train_data.to_csv('DF_Radiomics_noduls_with_diagnose_train_data_scaled.csv', index=False)\n",
    "    test_data.to_csv('DF_Radiomics_noduls_with_diagnose_test_data_scaled.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'data' is your pandas DataFrame\n",
    "# Ensure the DataFrame only contains numeric values\n",
    "data = pd.read_csv('DF_Radiomics_noduls_with_diagnose_train_data_scaled.csv')\n",
    "#drop patient and node columns\n",
    "data = data.drop(['Patient', 'Node'], axis=1)\n",
    "# TODO maybe add the columns later to see if it helps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and labels\n",
    "X = data.drop('Labels', axis=1).values\n",
    "y = data['Labels'].values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X).float()\n",
    "y_tensor = torch.tensor(y).float()\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, stratify=y_tensor, random_state=ran_seed)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: torch.Size([197, 115])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data:\", train_dataset.tensors[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Dropout for regularization\n",
    "            nn.Linear(hidden_size, hidden_size*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Dropout for regularization\n",
    "            nn.Linear(hidden_size*2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = train_dataset.tensors[0].shape[1]  # Get the number of features from your dataset\n",
    "hidden_size = input_size*2  # You can tune this\n",
    "output_size = 4   # 3 labels \n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 50  # Adjust based on your runtime requirement\n",
    "early_stopping_factor = 10\n",
    "clip_value = 1  # for gradient clipping\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = FCNN(input_size, hidden_size, output_size).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # L2 regularization\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "#check if cuda is available, print the gpu model name\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    # Move model to the device\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize previous loss to infinity for comparison in the first epoch\n",
    "# prev_loss = float('inf')\n",
    "# stop_training = False\n",
    "\n",
    "# # Training loop\n",
    "# model.fc = nn.Linear(input_size, output_size)  # 'num_output_neurons' is the number of output neurons in your linear layer\n",
    "\n",
    "# # Training loop\n",
    "# model.train()\n",
    "# for epoch in range(epochs):\n",
    "#     for inputs, targets in train_loader:\n",
    "    \n",
    "#         # Check for nan or inf values in inputs\n",
    "#         assert not torch.isnan(inputs).any(), \"NaN values found in inputs\"\n",
    "#         assert not torch.isinf(inputs).any(), \"Infinite values found in inputs\"\n",
    "        \n",
    "#         # Move inputs and targets to the device\n",
    "#         inputs = inputs.to(device)\n",
    "#         targets = targets.to(device)\n",
    "        \n",
    "#         targets = targets.long()\n",
    "        \n",
    "\n",
    "#         # Zero the gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model(inputs)\n",
    "\n",
    "#         # Compute loss\n",
    "#         loss = criterion(outputs, targets)\n",
    "        \n",
    "#         #naive early stopping\n",
    "#         # If the loss has increased substantially, stop training\n",
    "#         if loss.item() > prev_loss * early_stopping_factor:  # Adjust this factor as needed\n",
    "#             print(f\"Stopping training at epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")\n",
    "#             stop_training = True\n",
    "#             break\n",
    "        \n",
    "\n",
    "#         # Backward pass and optimize\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "        \n",
    "#         # Update previous loss\n",
    "#         prev_loss = loss.item()\n",
    "    \n",
    "#     if stop_training:\n",
    "#         break\n",
    "    \n",
    "\n",
    "    \n",
    "#     print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.15155739656516484\n",
      "Epoch 2/50, Loss: 0.1835748212678092\n",
      "Epoch 3/50, Loss: 0.20240314890231406\n",
      "Epoch 4/50, Loss: 0.12604104966989585\n",
      "Epoch 5/50, Loss: 0.09984725101717881\n",
      "Epoch 6/50, Loss: 0.12222789919802121\n",
      "Epoch 7/50, Loss: 0.1290846403156008\n",
      "Epoch 8/50, Loss: 0.14346522944314138\n",
      "Epoch 9/50, Loss: 0.12562836653419904\n",
      "Early stopping at epoch 10/50, best loss: 0.09984725101717881\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize best loss to infinity for comparison in the first epoch\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Patience counter\n",
    "patience_counter = 0\n",
    "\n",
    "# Patience limit\n",
    "patience_limit = 5\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "    \n",
    "        # Move inputs and targets to the device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        targets = targets.long()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Average epoch loss\n",
    "    epoch_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    # If the training loss has improved, save the model and reset the patience counter\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        # If the training loss has not improved, increment the patience counter\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience_limit:\n",
    "            print(f\"Early stopping at epoch {epoch+1}/{epochs}, best loss: {best_loss}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "## Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 84.0%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "\n",
    "            total_predictions += targets.size(0)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "# Use the function\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "accuracy = evaluate(model, test_loader, device)\n",
    "print(f'Accuracy: {accuracy * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.09677419354838%\n"
     ]
    }
   ],
   "source": [
    "validation_data = pd.read_csv('DF_Radiomics_noduls_with_diagnose_test_data_scaled.csv')\n",
    "\n",
    "#create the tensor dataset\n",
    "X_test = validation_data.drop(['Patient', 'Node', 'Labels'], axis=1).values\n",
    "y_test = validation_data['Labels'].values\n",
    "validation_dataset = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
    "\n",
    "# Use the function\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "accuracy = evaluate(model, validation_loader, device)\n",
    "print(f'Accuracy: {accuracy * 100}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
