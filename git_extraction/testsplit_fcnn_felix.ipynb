{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the CSV file into a DataFrame.\n",
    "2. Parse the \"Radiomics\" column, as it contains JSON data.\n",
    "3. Remove columns with the same values across all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(309, 103)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "# create random seed for reproducibility\n",
    "ran_seed = 42\n",
    "\n",
    "# Load the data from DF_Radiomics_noduls_with_diagnose.csv\n",
    "file_path = \"DF_Radiomics_noduls_with_diagnose.csv\"\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "# Convert the 'Labels' column to an integer\n",
    "data['Labels'] = data['Labels'].astype(int)\n",
    "\n",
    "# drop all rows where the label == 0\n",
    "data = data[data.Labels != 0]\n",
    "\n",
    "# Parse the JSON in the 'Radiomics' column\n",
    "data['Radiomics'] = data['Radiomics'].apply(json.loads)\n",
    "\n",
    "# Convert the 'Radiomics' column into separate columns\n",
    "radiomics_data = pd.json_normalize(data['Radiomics'])\n",
    "\n",
    "\n",
    "# Drop the original 'Radiomics' column\n",
    "data = data.drop('Radiomics', axis=1)\n",
    "\n",
    "\n",
    "# Reset the indices of both DataFrames\n",
    "data = data.reset_index(drop=True)\n",
    "radiomics_data = radiomics_data.reset_index(drop=True)\n",
    "\n",
    "# Combine the data with the new radiomics columns\n",
    "data = pd.concat([data, radiomics_data], axis=1)\n",
    "\n",
    "# Remove columns with the same value across all rows\n",
    "data = data.loc[:, (data != data.iloc[0]).any()]\n",
    "\n",
    "#remove columns with all NaN values\n",
    "data = data.dropna(axis=1, how='all')\n",
    "\n",
    "print(data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Node</th>\n",
       "      <th>Labels</th>\n",
       "      <th>diagnostics_Image-original_Hash</th>\n",
       "      <th>diagnostics_Image-original_Spacing</th>\n",
       "      <th>diagnostics_Image-original_Size</th>\n",
       "      <th>diagnostics_Image-original_Mean</th>\n",
       "      <th>diagnostics_Image-original_Minimum</th>\n",
       "      <th>diagnostics_Image-original_Maximum</th>\n",
       "      <th>diagnostics_Mask-original_Hash</th>\n",
       "      <th>...</th>\n",
       "      <th>original_gldm_GrayLevelNonUniformity</th>\n",
       "      <th>original_gldm_GrayLevelVariance</th>\n",
       "      <th>original_gldm_HighGrayLevelEmphasis</th>\n",
       "      <th>original_gldm_LargeDependenceEmphasis</th>\n",
       "      <th>original_gldm_LargeDependenceHighGrayLevelEmphasis</th>\n",
       "      <th>original_gldm_LargeDependenceLowGrayLevelEmphasis</th>\n",
       "      <th>original_gldm_LowGrayLevelEmphasis</th>\n",
       "      <th>original_gldm_SmallDependenceEmphasis</th>\n",
       "      <th>original_gldm_SmallDependenceHighGrayLevelEmphasis</th>\n",
       "      <th>original_gldm_SmallDependenceLowGrayLevelEmphasis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LIDC-IDRI-0068</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>3</td>\n",
       "      <td>bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28</td>\n",
       "      <td>[0.683594, 0.683594, 1.25]</td>\n",
       "      <td>[512, 512, 261]</td>\n",
       "      <td>-1026.065264</td>\n",
       "      <td>-3024.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>0506d1d0d6522eddd1640c8ea75c2fc5a9266270</td>\n",
       "      <td>...</td>\n",
       "      <td>7.355556</td>\n",
       "      <td>60.706173</td>\n",
       "      <td>469.644444</td>\n",
       "      <td>23.444444</td>\n",
       "      <td>16578.377778</td>\n",
       "      <td>0.053875</td>\n",
       "      <td>0.021012</td>\n",
       "      <td>0.488461</td>\n",
       "      <td>152.929922</td>\n",
       "      <td>0.019809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LIDC-IDRI-0068</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>3</td>\n",
       "      <td>bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28</td>\n",
       "      <td>[0.683594, 0.683594, 1.25]</td>\n",
       "      <td>[512, 512, 261]</td>\n",
       "      <td>-1026.065264</td>\n",
       "      <td>-3024.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>9d7da356d43e2f7ad7f374f6c193e97f6088d7c7</td>\n",
       "      <td>...</td>\n",
       "      <td>7.467153</td>\n",
       "      <td>72.801002</td>\n",
       "      <td>471.051095</td>\n",
       "      <td>17.496350</td>\n",
       "      <td>13573.328467</td>\n",
       "      <td>0.110650</td>\n",
       "      <td>0.024328</td>\n",
       "      <td>0.494688</td>\n",
       "      <td>165.356306</td>\n",
       "      <td>0.010062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LIDC-IDRI-0068</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>3</td>\n",
       "      <td>bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28</td>\n",
       "      <td>[0.683594, 0.683594, 1.25]</td>\n",
       "      <td>[512, 512, 261]</td>\n",
       "      <td>-1026.065264</td>\n",
       "      <td>-3024.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>c0a43747a23d26b107e21614525f2fd8870ffefc</td>\n",
       "      <td>...</td>\n",
       "      <td>7.685185</td>\n",
       "      <td>43.527006</td>\n",
       "      <td>277.787037</td>\n",
       "      <td>20.370370</td>\n",
       "      <td>9310.490741</td>\n",
       "      <td>0.084481</td>\n",
       "      <td>0.031811</td>\n",
       "      <td>0.463956</td>\n",
       "      <td>84.174037</td>\n",
       "      <td>0.027819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LIDC-IDRI-0068</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>3</td>\n",
       "      <td>bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28</td>\n",
       "      <td>[0.683594, 0.683594, 1.25]</td>\n",
       "      <td>[512, 512, 261]</td>\n",
       "      <td>-1026.065264</td>\n",
       "      <td>-3024.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>72a09dc3f5d5d146b13402b8ef109422cc3f38a5</td>\n",
       "      <td>...</td>\n",
       "      <td>6.780220</td>\n",
       "      <td>35.367709</td>\n",
       "      <td>229.219780</td>\n",
       "      <td>18.780220</td>\n",
       "      <td>7065.923077</td>\n",
       "      <td>0.084783</td>\n",
       "      <td>0.026368</td>\n",
       "      <td>0.465301</td>\n",
       "      <td>67.725183</td>\n",
       "      <td>0.021973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LIDC-IDRI-0072</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>1</td>\n",
       "      <td>54705f26f9320581c90452445aa820fe9630d5e9</td>\n",
       "      <td>[0.732422, 0.732422, 1.25]</td>\n",
       "      <td>[512, 512, 305]</td>\n",
       "      <td>-871.936330</td>\n",
       "      <td>-3024.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>05efcefff38c73903c3d7839bb987a49176f6068</td>\n",
       "      <td>...</td>\n",
       "      <td>629.334146</td>\n",
       "      <td>45.147393</td>\n",
       "      <td>1253.131545</td>\n",
       "      <td>28.918031</td>\n",
       "      <td>43475.541623</td>\n",
       "      <td>0.020967</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.262518</td>\n",
       "      <td>254.476429</td>\n",
       "      <td>0.000632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Patient     Node  Labels           diagnostics_Image-original_Hash  \\\n",
       "0  LIDC-IDRI-0068  Node_N1       3  bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28   \n",
       "1  LIDC-IDRI-0068  Node_N1       3  bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28   \n",
       "2  LIDC-IDRI-0068  Node_N1       3  bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28   \n",
       "3  LIDC-IDRI-0068  Node_N1       3  bea2c9750ea59a0bebb6d3bd63ffacc40fcf6a28   \n",
       "4  LIDC-IDRI-0072  Node_N1       1  54705f26f9320581c90452445aa820fe9630d5e9   \n",
       "\n",
       "  diagnostics_Image-original_Spacing diagnostics_Image-original_Size  \\\n",
       "0         [0.683594, 0.683594, 1.25]                 [512, 512, 261]   \n",
       "1         [0.683594, 0.683594, 1.25]                 [512, 512, 261]   \n",
       "2         [0.683594, 0.683594, 1.25]                 [512, 512, 261]   \n",
       "3         [0.683594, 0.683594, 1.25]                 [512, 512, 261]   \n",
       "4         [0.732422, 0.732422, 1.25]                 [512, 512, 305]   \n",
       "\n",
       "   diagnostics_Image-original_Mean  diagnostics_Image-original_Minimum  \\\n",
       "0                     -1026.065264                             -3024.0   \n",
       "1                     -1026.065264                             -3024.0   \n",
       "2                     -1026.065264                             -3024.0   \n",
       "3                     -1026.065264                             -3024.0   \n",
       "4                      -871.936330                             -3024.0   \n",
       "\n",
       "   diagnostics_Image-original_Maximum  \\\n",
       "0                              3071.0   \n",
       "1                              3071.0   \n",
       "2                              3071.0   \n",
       "3                              3071.0   \n",
       "4                              3071.0   \n",
       "\n",
       "             diagnostics_Mask-original_Hash  ...  \\\n",
       "0  0506d1d0d6522eddd1640c8ea75c2fc5a9266270  ...   \n",
       "1  9d7da356d43e2f7ad7f374f6c193e97f6088d7c7  ...   \n",
       "2  c0a43747a23d26b107e21614525f2fd8870ffefc  ...   \n",
       "3  72a09dc3f5d5d146b13402b8ef109422cc3f38a5  ...   \n",
       "4  05efcefff38c73903c3d7839bb987a49176f6068  ...   \n",
       "\n",
       "  original_gldm_GrayLevelNonUniformity original_gldm_GrayLevelVariance  \\\n",
       "0                             7.355556                       60.706173   \n",
       "1                             7.467153                       72.801002   \n",
       "2                             7.685185                       43.527006   \n",
       "3                             6.780220                       35.367709   \n",
       "4                           629.334146                       45.147393   \n",
       "\n",
       "  original_gldm_HighGrayLevelEmphasis  original_gldm_LargeDependenceEmphasis  \\\n",
       "0                          469.644444                              23.444444   \n",
       "1                          471.051095                              17.496350   \n",
       "2                          277.787037                              20.370370   \n",
       "3                          229.219780                              18.780220   \n",
       "4                         1253.131545                              28.918031   \n",
       "\n",
       "   original_gldm_LargeDependenceHighGrayLevelEmphasis  \\\n",
       "0                                       16578.377778    \n",
       "1                                       13573.328467    \n",
       "2                                        9310.490741    \n",
       "3                                        7065.923077    \n",
       "4                                       43475.541623    \n",
       "\n",
       "  original_gldm_LargeDependenceLowGrayLevelEmphasis  \\\n",
       "0                                          0.053875   \n",
       "1                                          0.110650   \n",
       "2                                          0.084481   \n",
       "3                                          0.084783   \n",
       "4                                          0.020967   \n",
       "\n",
       "  original_gldm_LowGrayLevelEmphasis  original_gldm_SmallDependenceEmphasis  \\\n",
       "0                           0.021012                               0.488461   \n",
       "1                           0.024328                               0.494688   \n",
       "2                           0.031811                               0.463956   \n",
       "3                           0.026368                               0.465301   \n",
       "4                           0.001319                               0.262518   \n",
       "\n",
       "   original_gldm_SmallDependenceHighGrayLevelEmphasis  \\\n",
       "0                                         152.929922    \n",
       "1                                         165.356306    \n",
       "2                                          84.174037    \n",
       "3                                          67.725183    \n",
       "4                                         254.476429    \n",
       "\n",
       "   original_gldm_SmallDependenceLowGrayLevelEmphasis  \n",
       "0                                           0.019809  \n",
       "1                                           0.010062  \n",
       "2                                           0.027819  \n",
       "3                                           0.021973  \n",
       "4                                           0.000632  \n",
       "\n",
       "[5 rows x 103 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove hash columns\n",
    "data = data.drop(['diagnostics_Image-original_Hash', 'diagnostics_Mask-original_Hash'], axis=1)\n",
    "\n",
    "# ok looks like all the objeckt columns except of \"Patient\" & \"Node\" are in this form [0.683594, 0.683594, 1.25] which is a list of multiple floats\n",
    "# exploade them into multiple columns\n",
    "\n",
    "object_columns = data.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# Remove 'Patient' and 'Node' from the list\n",
    "object_columns.remove('Patient')\n",
    "object_columns.remove('Node')\n",
    "\n",
    "# Explode the lists in each object column into multiple columns\n",
    "for column in object_columns:\n",
    "    # Convert each list to a Series and expand it into multiple columns\n",
    "    expanded_columns = data[column].apply(pd.Series)\n",
    "    \n",
    "    # Rename the expanded columns to have the original column name as a prefix\n",
    "    expanded_columns = expanded_columns.rename(columns=lambda x: f\"{column}_{x}\")\n",
    "    \n",
    "    # Drop the original column from the DataFrame\n",
    "    data = data.drop(column, axis=1)\n",
    "    \n",
    "    # Concatenate the expanded columns to the DataFrame\n",
    "    data = pd.concat([data, expanded_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already exist, skipping this step\n"
     ]
    }
   ],
   "source": [
    "# Create a stratified split\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['Labels'], random_state=ran_seed)\n",
    "\n",
    "# if the files already exist, skip this step\n",
    "if os.path.isfile('DF_Radiomics_noduls_with_diagnose_train_data.csv') and os.path.isfile('DF_Radiomics_noduls_with_diagnose_test_data.csv'):\n",
    "    print(\"Files already exist, skipping this step\")\n",
    "else:\n",
    "    # Save the data to CSV files\n",
    "    train_data.to_csv('DF_Radiomics_noduls_with_diagnose_train_data.csv', index=False)\n",
    "    test_data.to_csv('DF_Radiomics_noduls_with_diagnose_test_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (247, 118)\n",
      "Test data: (62, 118)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data:\", train_data.shape)\n",
    "print(\"Test data:\", test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Patient</th>\n",
       "      <th>Node</th>\n",
       "      <th>Labels</th>\n",
       "      <th>diagnostics_Image-original_Mean</th>\n",
       "      <th>diagnostics_Image-original_Minimum</th>\n",
       "      <th>diagnostics_Image-original_Maximum</th>\n",
       "      <th>diagnostics_Mask-original_VoxelNum</th>\n",
       "      <th>diagnostics_Mask-original_VolumeNum</th>\n",
       "      <th>original_firstorder_10Percentile</th>\n",
       "      <th>original_firstorder_90Percentile</th>\n",
       "      <th>...</th>\n",
       "      <th>diagnostics_Mask-original_BoundingBox_2</th>\n",
       "      <th>diagnostics_Mask-original_BoundingBox_3</th>\n",
       "      <th>diagnostics_Mask-original_BoundingBox_4</th>\n",
       "      <th>diagnostics_Mask-original_BoundingBox_5</th>\n",
       "      <th>diagnostics_Mask-original_CenterOfMassIndex_0</th>\n",
       "      <th>diagnostics_Mask-original_CenterOfMassIndex_1</th>\n",
       "      <th>diagnostics_Mask-original_CenterOfMassIndex_2</th>\n",
       "      <th>diagnostics_Mask-original_CenterOfMass_0</th>\n",
       "      <th>diagnostics_Mask-original_CenterOfMass_1</th>\n",
       "      <th>diagnostics_Mask-original_CenterOfMass_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LIDC-IDRI-0137</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>3</td>\n",
       "      <td>-671.885608</td>\n",
       "      <td>-2048.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>175.5</td>\n",
       "      <td>850.5</td>\n",
       "      <td>...</td>\n",
       "      <td>30</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>332.692308</td>\n",
       "      <td>389.538462</td>\n",
       "      <td>30.307692</td>\n",
       "      <td>53.215868</td>\n",
       "      <td>83.626926</td>\n",
       "      <td>-321.730769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>LIDC-IDRI-0377</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>2</td>\n",
       "      <td>-882.321409</td>\n",
       "      <td>-3024.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>2402</td>\n",
       "      <td>1</td>\n",
       "      <td>-307.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>...</td>\n",
       "      <td>169</td>\n",
       "      <td>29</td>\n",
       "      <td>24</td>\n",
       "      <td>9</td>\n",
       "      <td>382.402998</td>\n",
       "      <td>308.854288</td>\n",
       "      <td>173.039550</td>\n",
       "      <td>92.739302</td>\n",
       "      <td>28.898399</td>\n",
       "      <td>-68.460564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>LIDC-IDRI-0167</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>1</td>\n",
       "      <td>-664.766231</td>\n",
       "      <td>-2048.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>-444.5</td>\n",
       "      <td>-66.5</td>\n",
       "      <td>...</td>\n",
       "      <td>50</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>70.267857</td>\n",
       "      <td>174.964286</td>\n",
       "      <td>50.321429</td>\n",
       "      <td>-136.237780</td>\n",
       "      <td>-53.812866</td>\n",
       "      <td>-234.696429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>LIDC-IDRI-0272</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>3</td>\n",
       "      <td>-824.358062</td>\n",
       "      <td>-2048.0</td>\n",
       "      <td>3071.0</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "      <td>-447.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>...</td>\n",
       "      <td>81</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>209.313725</td>\n",
       "      <td>390.941176</td>\n",
       "      <td>81.568627</td>\n",
       "      <td>-47.673652</td>\n",
       "      <td>80.722794</td>\n",
       "      <td>-109.078431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>LIDC-IDRI-0234</td>\n",
       "      <td>Node_N1</td>\n",
       "      <td>1</td>\n",
       "      <td>-708.012378</td>\n",
       "      <td>-2048.0</td>\n",
       "      <td>3029.0</td>\n",
       "      <td>251</td>\n",
       "      <td>1</td>\n",
       "      <td>-569.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>...</td>\n",
       "      <td>41</td>\n",
       "      <td>11</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>367.756972</td>\n",
       "      <td>310.848606</td>\n",
       "      <td>41.689243</td>\n",
       "      <td>65.179121</td>\n",
       "      <td>43.765426</td>\n",
       "      <td>-236.276892</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Patient     Node  Labels  diagnostics_Image-original_Mean  \\\n",
       "23   LIDC-IDRI-0137  Node_N1       3                      -671.885608   \n",
       "263  LIDC-IDRI-0377  Node_N1       2                      -882.321409   \n",
       "44   LIDC-IDRI-0167  Node_N1       1                      -664.766231   \n",
       "219  LIDC-IDRI-0272  Node_N1       3                      -824.358062   \n",
       "143  LIDC-IDRI-0234  Node_N1       1                      -708.012378   \n",
       "\n",
       "     diagnostics_Image-original_Minimum  diagnostics_Image-original_Maximum  \\\n",
       "23                              -2048.0                              3071.0   \n",
       "263                             -3024.0                              3071.0   \n",
       "44                              -2048.0                              3071.0   \n",
       "219                             -2048.0                              3071.0   \n",
       "143                             -2048.0                              3029.0   \n",
       "\n",
       "     diagnostics_Mask-original_VoxelNum  diagnostics_Mask-original_VolumeNum  \\\n",
       "23                                   26                                    1   \n",
       "263                                2402                                    1   \n",
       "44                                   56                                    1   \n",
       "219                                  51                                    1   \n",
       "143                                 251                                    1   \n",
       "\n",
       "     original_firstorder_10Percentile  original_firstorder_90Percentile  ...  \\\n",
       "23                              175.5                             850.5  ...   \n",
       "263                            -307.0                              61.0  ...   \n",
       "44                             -444.5                             -66.5  ...   \n",
       "219                            -447.0                             102.0  ...   \n",
       "143                            -569.0                              82.0  ...   \n",
       "\n",
       "     diagnostics_Mask-original_BoundingBox_2  \\\n",
       "23                                        30   \n",
       "263                                      169   \n",
       "44                                        50   \n",
       "219                                       81   \n",
       "143                                       41   \n",
       "\n",
       "     diagnostics_Mask-original_BoundingBox_3  \\\n",
       "23                                         4   \n",
       "263                                       29   \n",
       "44                                         6   \n",
       "219                                        6   \n",
       "143                                       11   \n",
       "\n",
       "     diagnostics_Mask-original_BoundingBox_4  \\\n",
       "23                                         6   \n",
       "263                                       24   \n",
       "44                                         9   \n",
       "219                                        7   \n",
       "143                                       14   \n",
       "\n",
       "     diagnostics_Mask-original_BoundingBox_5  \\\n",
       "23                                         2   \n",
       "263                                        9   \n",
       "44                                         2   \n",
       "219                                        2   \n",
       "143                                        3   \n",
       "\n",
       "     diagnostics_Mask-original_CenterOfMassIndex_0  \\\n",
       "23                                      332.692308   \n",
       "263                                     382.402998   \n",
       "44                                       70.267857   \n",
       "219                                     209.313725   \n",
       "143                                     367.756972   \n",
       "\n",
       "     diagnostics_Mask-original_CenterOfMassIndex_1  \\\n",
       "23                                      389.538462   \n",
       "263                                     308.854288   \n",
       "44                                      174.964286   \n",
       "219                                     390.941176   \n",
       "143                                     310.848606   \n",
       "\n",
       "     diagnostics_Mask-original_CenterOfMassIndex_2  \\\n",
       "23                                       30.307692   \n",
       "263                                     173.039550   \n",
       "44                                       50.321429   \n",
       "219                                      81.568627   \n",
       "143                                      41.689243   \n",
       "\n",
       "     diagnostics_Mask-original_CenterOfMass_0  \\\n",
       "23                                  53.215868   \n",
       "263                                 92.739302   \n",
       "44                                -136.237780   \n",
       "219                                -47.673652   \n",
       "143                                 65.179121   \n",
       "\n",
       "     diagnostics_Mask-original_CenterOfMass_1  \\\n",
       "23                                  83.626926   \n",
       "263                                 28.898399   \n",
       "44                                 -53.812866   \n",
       "219                                 80.722794   \n",
       "143                                 43.765426   \n",
       "\n",
       "     diagnostics_Mask-original_CenterOfMass_2  \n",
       "23                                -321.730769  \n",
       "263                                -68.460564  \n",
       "44                                -234.696429  \n",
       "219                               -109.078431  \n",
       "143                               -236.276892  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data already exists\n"
     ]
    }
   ],
   "source": [
    "# if DF_Radiomics_noduls_with_diagnose_train_data_scaled.csv and DF_Radiomics_noduls_with_diagnose_test_data_scaled.csv already exist, skip this step\n",
    "# otherwise scale the data and save it to CSV files\n",
    "if os.path.isfile('DF_Radiomics_noduls_with_diagnose_train_data_scaled.csv') and os.path.isfile('DF_Radiomics_noduls_with_diagnose_test_data_scaled.csv'):\n",
    "    print(\"Scaled data already exists\")\n",
    "else:\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    # Get all column names\n",
    "    all_columns = train_data.columns.tolist()\n",
    "\n",
    "    # Exclude the first three columns\n",
    "    features = all_columns[3:]\n",
    "\n",
    "    # Create a stratified split\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, stratify=data['Labels'])\n",
    "\n",
    "    # Create a scaler\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    # Fit the scaler on the training data and transform both training and test data\n",
    "    train_data[features] = scaler.fit_transform(train_data[features])\n",
    "    test_data[features] = scaler.transform(test_data[features])\n",
    "\n",
    "    # Save the data to CSV files\n",
    "    train_data.to_csv('DF_Radiomics_noduls_with_diagnose_train_data_scaled.csv', index=False)\n",
    "    test_data.to_csv('DF_Radiomics_noduls_with_diagnose_test_data_scaled.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully Connected Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'data' is your pandas DataFrame\n",
    "# Ensure the DataFrame only contains numeric values\n",
    "data = pd.read_csv('DF_Radiomics_noduls_with_diagnose_train_data_scaled.csv')\n",
    "#drop patient and node columns\n",
    "data = data.drop(['Patient', 'Node'], axis=1)\n",
    "# TODO maybe add the columns later to see if it helps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trainset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into features and labels\n",
    "X = data.drop('Labels', axis=1).values\n",
    "y = data['Labels'].values\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_tensor = torch.tensor(X).float()\n",
    "y_tensor = torch.tensor(y).float()\n",
    "\n",
    "# Stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tensor, y_tensor, test_size=0.2, stratify=y_tensor, random_state=ran_seed)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: torch.Size([197, 115])\n"
     ]
    }
   ],
   "source": [
    "print(\"Train data:\", train_dataset.tensors[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class FCNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FCNN, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Dropout for regularization\n",
    "            nn.Linear(hidden_size, hidden_size*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),  # Dropout for regularization\n",
    "            nn.Linear(hidden_size*2, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = train_dataset.tensors[0].shape[1]  # Get the number of features from your dataset\n",
    "hidden_size = input_size*2  # You can tune this\n",
    "output_size = 4   # 3 labels \n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "epochs = 50  # Adjust based on your runtime requirement\n",
    "early_stopping_factor = 10\n",
    "clip_value = 1  # for gradient clipping\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "model = FCNN(input_size, hidden_size, output_size).cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)  # L2 regularization\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "#check if cuda is available, print the gpu model name\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == \"cuda\":\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    # Move model to the device\n",
    "    model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.3304967199053084\n",
      "Epoch 2/50, Loss: 1.1353100197655814\n",
      "Epoch 3/50, Loss: 1.0851800782339913\n",
      "Epoch 4/50, Loss: 0.9991901431764875\n",
      "Epoch 5/50, Loss: 0.87434093441282\n",
      "Epoch 6/50, Loss: 0.7975443005561829\n",
      "Epoch 7/50, Loss: 0.9110166685921806\n",
      "Epoch 8/50, Loss: 0.6931230000087193\n",
      "Epoch 9/50, Loss: 0.662830039858818\n",
      "Epoch 10/50, Loss: 0.5734744497707912\n",
      "Epoch 11/50, Loss: 0.4815047766481127\n",
      "Epoch 12/50, Loss: 0.4496862547738211\n",
      "Epoch 13/50, Loss: 0.3978220011506762\n",
      "Epoch 14/50, Loss: 0.3232405888182776\n",
      "Epoch 15/50, Loss: 0.34018705572400776\n",
      "Epoch 16/50, Loss: 0.31875818754945484\n",
      "Epoch 17/50, Loss: 0.3030732146331242\n",
      "Epoch 18/50, Loss: 0.24767346041543142\n",
      "Epoch 19/50, Loss: 0.18171399193150656\n",
      "Epoch 20/50, Loss: 0.22024099635226385\n",
      "Epoch 21/50, Loss: 0.21861376400504792\n",
      "Epoch 22/50, Loss: 0.10599877632090024\n",
      "Epoch 23/50, Loss: 0.10694949488554682\n",
      "Epoch 24/50, Loss: 0.08373230908598218\n",
      "Epoch 25/50, Loss: 0.06051187333650887\n",
      "Epoch 26/50, Loss: 0.07139431046588081\n",
      "Epoch 27/50, Loss: 0.035258932039141655\n",
      "Epoch 28/50, Loss: 0.024580760725906918\n",
      "Epoch 29/50, Loss: 0.045731384373669116\n",
      "Epoch 30/50, Loss: 0.023450508090068718\n",
      "Epoch 31/50, Loss: 0.04406364734417626\n",
      "Epoch 32/50, Loss: 0.028461014486051033\n",
      "Epoch 33/50, Loss: 0.07199594069139234\n",
      "Epoch 34/50, Loss: 0.05093863852588194\n",
      "Early stopping at epoch 35/50, best loss: 0.023450508090068718\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize best loss to infinity for comparison in the first epoch\n",
    "best_loss = float('inf')\n",
    "\n",
    "# Patience counter\n",
    "patience_counter = 0\n",
    "\n",
    "# Patience limit\n",
    "patience_limit = 5\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for inputs, targets in train_loader:\n",
    "    \n",
    "        # Move inputs and targets to the device\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        targets = targets.long()\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # Average epoch loss\n",
    "    epoch_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "    # If the training loss has improved, save the model and reset the patience counter\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        # If the training loss has not improved, increment the patience counter\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience_limit:\n",
    "            print(f\"Early stopping at epoch {epoch+1}/{epochs}, best loss: {best_loss}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}\")\n",
    "\n",
    "# Load the best model\n",
    "model.load_state_dict(torch.load('best_model.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate\n",
    "\n",
    "Since Confusion Matrix is 3x3 calculate Sensitivity & Specificity for each class by considering that class as the positive class and the other two as the negative class.\n",
    "\n",
    "Sensitivity, also known as the true positive rate (TPR), measures the proportion of actual positives that are correctly identified as such. In other words, it measures the ability of the model to correctly identify positive instances.\n",
    "\n",
    "Specificity, on the other hand, measures the proportion of actual negatives that are correctly identified as such. It measures the ability of the model to correctly identify negative instances.\n",
    "\n",
    "The false positive rate (FPR) is the complement of specificity. It measures the proportion of actual negatives that are incorrectly identified as positives. In other words, it measures the rate at which the model makes false alarms.\n",
    "\n",
    "Here's how they relate:\n",
    "\n",
    "- TPR = Sensitivity = TP / (TP + FN)\n",
    "- FPR = 1 - Specificity = FP / (FP + TN)\n",
    "- Specificity = TN / (TN + FP)\n",
    "\n",
    "Where:\n",
    "- TP = True Positives\n",
    "- FN = False Negatives\n",
    "- FP = False Positives\n",
    "- TN = True Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    all_targets = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculations\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)  # Get the index of the max log-probability\n",
    "\n",
    "            total_predictions += targets.size(0)\n",
    "            correct_predictions += (predicted == targets).sum().item()\n",
    "\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "\n",
    "    # Calculate sensitivity and specificity for each class\n",
    "    sensitivity = np.diag(cm) / np.sum(cm, axis = 1)\n",
    "    specificity = (np.sum(cm) - np.sum(cm, axis = 0) - np.sum(cm, axis = 1) + np.diag(cm)) / (np.sum(cm) - np.sum(cm, axis = 0))\n",
    "\n",
    "    return accuracy, sensitivity, specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 78.0%\n",
      "Class 0: Sensitivity: 75.0%, Specificity: 89.1891891891892%\n",
      "Class 1: Sensitivity: 76.92307692307693%, Specificity: 90.9090909090909%\n",
      "Class 2: Sensitivity: 80.95238095238095%, Specificity: 86.66666666666667%\n"
     ]
    }
   ],
   "source": [
    "# Use the function\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "accuracy, sensitivity, specificity = evaluate(model, test_loader, device)\n",
    "print(f'Accuracy: {accuracy * 100}%')\n",
    "for i, (sens, spec) in enumerate(zip(sensitivity, specificity)):\n",
    "    print(f'Class {i}: Sensitivity: {sens * 100}%, Specificity: {spec * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 88.70967741935483%\n",
      "Class 0: Sensitivity: 95.0%, Specificity: 97.5609756097561%\n",
      "Class 1: Sensitivity: 87.5%, Specificity: 95.55555555555556%\n",
      "Class 2: Sensitivity: 84.61538461538461%, Specificity: 89.47368421052632%\n"
     ]
    }
   ],
   "source": [
    "validation_data = pd.read_csv('DF_Radiomics_noduls_with_diagnose_test_data_scaled.csv')\n",
    "\n",
    "#create the tensor dataset\n",
    "X_test = validation_data.drop(['Patient', 'Node', 'Labels'], axis=1).values\n",
    "y_test = validation_data['Labels'].values\n",
    "validation_dataset = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
    "\n",
    "# Use the function\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "accuracy, sensitivity, specificity = evaluate(model, validation_loader, device)\n",
    "print(f'Accuracy: {accuracy * 100}%')\n",
    "for i, (sens, spec) in enumerate(zip(sensitivity, specificity)):\n",
    "    print(f'Class {i}: Sensitivity: {sens * 100}%, Specificity: {spec * 100}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.3779354436056954\n",
      "Epoch 2/50, Loss: 1.3421578407287598\n",
      "Epoch 3/50, Loss: 1.3160295145852225\n",
      "Epoch 4/50, Loss: 1.2681296042033605\n",
      "Epoch 5/50, Loss: 1.232401932988848\n",
      "Epoch 6/50, Loss: 1.1976555926459176\n",
      "Epoch 7/50, Loss: 1.1565898997443063\n",
      "Epoch 8/50, Loss: 1.1289265666689192\n",
      "Epoch 9/50, Loss: 1.1114522559302193\n",
      "Epoch 10/50, Loss: 1.0388939210346766\n",
      "Epoch 11/50, Loss: 1.021112757069724\n",
      "Epoch 12/50, Loss: 1.026507556438446\n",
      "Epoch 13/50, Loss: 0.9901436652456012\n",
      "Epoch 14/50, Loss: 0.9268379552023751\n",
      "Epoch 15/50, Loss: 0.9635300976889474\n",
      "Epoch 16/50, Loss: 0.8990778071539742\n",
      "Epoch 17/50, Loss: 0.8695678796086993\n",
      "Epoch 18/50, Loss: 0.8551375355039325\n",
      "Epoch 19/50, Loss: 0.7835171478135246\n",
      "Epoch 20/50, Loss: 0.7730300256184169\n",
      "Epoch 21/50, Loss: 0.7588641047477722\n",
      "Epoch 22/50, Loss: 0.750460548060281\n",
      "Epoch 23/50, Loss: 0.7231575506074088\n",
      "Epoch 24/50, Loss: 0.6997528076171875\n",
      "Epoch 25/50, Loss: 0.6496872391019549\n",
      "Epoch 26/50, Loss: 0.7131269148417881\n",
      "Epoch 27/50, Loss: 0.6161242042269025\n",
      "Epoch 28/50, Loss: 0.6385850523199353\n",
      "Epoch 29/50, Loss: 0.6144531496933529\n",
      "Epoch 30/50, Loss: 0.6143340468406677\n",
      "Epoch 31/50, Loss: 0.6439550476414817\n",
      "Epoch 32/50, Loss: 0.5073503426143101\n",
      "Epoch 33/50, Loss: 0.5015573884759631\n",
      "Epoch 34/50, Loss: 0.6205182288374219\n",
      "Epoch 35/50, Loss: 0.481397134917123\n",
      "Epoch 36/50, Loss: 0.44834931407655987\n",
      "Epoch 37/50, Loss: 0.46376653228487286\n",
      "Epoch 38/50, Loss: 0.43965750081198557\n",
      "Epoch 39/50, Loss: 0.3729391545057297\n",
      "Epoch 40/50, Loss: 0.3709131266389574\n",
      "Epoch 41/50, Loss: 0.4049466039453234\n",
      "Epoch 42/50, Loss: 0.35963145749909536\n",
      "Epoch 43/50, Loss: 0.2977644657450063\n",
      "Epoch 44/50, Loss: 0.34442294921193806\n",
      "Epoch 45/50, Loss: 0.3617459386587143\n",
      "Epoch 46/50, Loss: 0.2628499780382429\n",
      "Epoch 47/50, Loss: 0.24134469670908792\n",
      "Epoch 48/50, Loss: 0.24769838154315948\n",
      "Epoch 49/50, Loss: 0.29524088757378714\n",
      "Epoch 50/50, Loss: 0.20514678955078125\n",
      "Epoch 1/50, Loss: 1.3727954626083374\n",
      "Epoch 2/50, Loss: 1.3406038965497697\n",
      "Epoch 3/50, Loss: 1.3042880637305123\n",
      "Epoch 4/50, Loss: 1.2662664822169714\n",
      "Epoch 5/50, Loss: 1.2206919022968836\n",
      "Epoch 6/50, Loss: 1.1819640227726527\n",
      "Epoch 7/50, Loss: 1.1368348939078194\n",
      "Epoch 8/50, Loss: 1.1074661357062203\n",
      "Epoch 9/50, Loss: 1.0976543256214686\n",
      "Epoch 10/50, Loss: 1.0518270134925842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_615/3107658289.py:105: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results = pd.concat([results, pd.DataFrame([[epoch, epoch_loss, best_loss, hidden_size, learning_rate, weight_decay, accuracy, sensitivity, specificity]], columns=results.columns)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/50, Loss: 1.0272281680788313\n",
      "Epoch 12/50, Loss: 0.9932590041841779\n",
      "Epoch 13/50, Loss: 0.9979591029030936\n",
      "Epoch 14/50, Loss: 0.9793221269335065\n",
      "Epoch 15/50, Loss: 0.9294391615050179\n",
      "Epoch 16/50, Loss: 0.9041635394096375\n",
      "Epoch 17/50, Loss: 0.8433543017932347\n",
      "Epoch 18/50, Loss: 0.7912102511950901\n",
      "Epoch 19/50, Loss: 0.8612799559320722\n",
      "Epoch 20/50, Loss: 0.8001096163477216\n",
      "Epoch 21/50, Loss: 0.7574533479554313\n",
      "Epoch 22/50, Loss: 0.8634302360670907\n",
      "Epoch 23/50, Loss: 0.7931511998176575\n",
      "Epoch 24/50, Loss: 0.6860793488366264\n",
      "Epoch 25/50, Loss: 0.6872975826263428\n",
      "Epoch 26/50, Loss: 0.6372499508517129\n",
      "Epoch 27/50, Loss: 0.6614081135817936\n",
      "Epoch 28/50, Loss: 0.6357598773070744\n",
      "Epoch 29/50, Loss: 0.5535514567579541\n",
      "Epoch 30/50, Loss: 0.6209481869425092\n",
      "Epoch 31/50, Loss: 0.5285731085709163\n",
      "Epoch 32/50, Loss: 0.5344104255948748\n",
      "Epoch 33/50, Loss: 0.5277976053101676\n",
      "Epoch 34/50, Loss: 0.48495069997651236\n",
      "Epoch 35/50, Loss: 0.47730797954968046\n",
      "Epoch 36/50, Loss: 0.4837290474346706\n",
      "Epoch 37/50, Loss: 0.4901323403630938\n",
      "Epoch 38/50, Loss: 0.40381305132593426\n",
      "Epoch 39/50, Loss: 0.4094332754611969\n",
      "Epoch 40/50, Loss: 0.3930794043200357\n",
      "Epoch 41/50, Loss: 0.3593614825180599\n",
      "Epoch 42/50, Loss: 0.33557045246873585\n",
      "Epoch 43/50, Loss: 0.3385778559105737\n",
      "Epoch 44/50, Loss: 0.27887171506881714\n",
      "Epoch 45/50, Loss: 0.2694737102304186\n",
      "Epoch 46/50, Loss: 0.27516080226217\n",
      "Epoch 47/50, Loss: 0.34453517837183817\n",
      "Epoch 48/50, Loss: 0.2739106799874987\n",
      "Epoch 49/50, Loss: 0.2318741168294634\n",
      "Epoch 50/50, Loss: 0.26951688740934643\n",
      "Epoch 1/50, Loss: 1.387892178126744\n",
      "Epoch 2/50, Loss: 1.3475018569401331\n",
      "Epoch 3/50, Loss: 1.3192709514072962\n",
      "Epoch 4/50, Loss: 1.2750794887542725\n",
      "Epoch 5/50, Loss: 1.2405582666397095\n",
      "Epoch 6/50, Loss: 1.1963224070412772\n",
      "Epoch 7/50, Loss: 1.1611687796456474\n",
      "Epoch 8/50, Loss: 1.1020819459642683\n",
      "Epoch 9/50, Loss: 1.0734559467860632\n",
      "Epoch 10/50, Loss: 1.0697365318025862\n",
      "Epoch 11/50, Loss: 1.0337541018213545\n",
      "Epoch 12/50, Loss: 0.9680364813123431\n",
      "Epoch 13/50, Loss: 0.9553897636277335\n",
      "Epoch 14/50, Loss: 0.9019871183804103\n",
      "Epoch 15/50, Loss: 0.9070480465888977\n",
      "Epoch 16/50, Loss: 0.8895615765026638\n",
      "Epoch 17/50, Loss: 0.8446606738226754\n",
      "Epoch 18/50, Loss: 0.8253900068146842\n",
      "Epoch 19/50, Loss: 0.8627784422465733\n",
      "Epoch 20/50, Loss: 0.7307168756212506\n",
      "Epoch 21/50, Loss: 0.7769190328461784\n",
      "Epoch 22/50, Loss: 0.7114820906094143\n",
      "Epoch 23/50, Loss: 0.6835292833192008\n",
      "Epoch 24/50, Loss: 0.7614188960620335\n",
      "Epoch 25/50, Loss: 0.7006227544375828\n",
      "Epoch 26/50, Loss: 0.7023158414023263\n",
      "Epoch 27/50, Loss: 0.5876791434628623\n",
      "Epoch 28/50, Loss: 0.5978143385478428\n",
      "Epoch 29/50, Loss: 0.5797137064593179\n",
      "Epoch 30/50, Loss: 0.5809420900685447\n",
      "Epoch 31/50, Loss: 0.5410969597952706\n",
      "Epoch 32/50, Loss: 0.5522309754576001\n",
      "Epoch 33/50, Loss: 0.553265916449683\n",
      "Epoch 34/50, Loss: 0.4667119213512966\n",
      "Epoch 35/50, Loss: 0.4522598556109837\n",
      "Epoch 36/50, Loss: 0.4051036068371364\n",
      "Epoch 37/50, Loss: 0.41187878804547445\n",
      "Epoch 38/50, Loss: 0.3875160004411425\n",
      "Epoch 39/50, Loss: 0.3816668859549931\n",
      "Epoch 40/50, Loss: 0.37478369048663546\n",
      "Epoch 41/50, Loss: 0.36749571561813354\n",
      "Epoch 42/50, Loss: 0.29432234168052673\n",
      "Epoch 43/50, Loss: 0.31006400287151337\n",
      "Epoch 44/50, Loss: 0.3077293911150524\n",
      "Epoch 45/50, Loss: 0.30941379921776907\n",
      "Epoch 46/50, Loss: 0.255603164434433\n",
      "Epoch 47/50, Loss: 0.27215169795921873\n",
      "Epoch 48/50, Loss: 0.2494947197181838\n",
      "Epoch 49/50, Loss: 0.2214393871171134\n",
      "Epoch 50/50, Loss: 0.22583094026361192\n",
      "Epoch 1/50, Loss: 1.2520678043365479\n",
      "Epoch 2/50, Loss: 1.0853880643844604\n",
      "Epoch 3/50, Loss: 0.8724613615444728\n",
      "Epoch 4/50, Loss: 0.8819925103868756\n",
      "Epoch 5/50, Loss: 0.7394804699080331\n",
      "Epoch 6/50, Loss: 0.5987931404794965\n",
      "Epoch 7/50, Loss: 0.6087302224976676\n",
      "Epoch 8/50, Loss: 0.4557710587978363\n",
      "Epoch 9/50, Loss: 0.37338072061538696\n",
      "Epoch 10/50, Loss: 0.2922464204686029\n",
      "Epoch 11/50, Loss: 0.3218911056007658\n",
      "Epoch 12/50, Loss: 0.1807968584554536\n",
      "Epoch 13/50, Loss: 0.1791789193770715\n",
      "Epoch 14/50, Loss: 0.10395024078232902\n",
      "Epoch 15/50, Loss: 0.15015154970543726\n",
      "Epoch 16/50, Loss: 0.07855212848101344\n",
      "Epoch 17/50, Loss: 0.07452240546366998\n",
      "Epoch 18/50, Loss: 0.08052739673959357\n",
      "Epoch 19/50, Loss: 0.07811760030952948\n",
      "Epoch 20/50, Loss: 0.09685293683183513\n",
      "Epoch 21/50, Loss: 0.08827528450638056\n",
      "Epoch 22/50, Loss: 0.033842437651141415\n",
      "Epoch 23/50, Loss: 0.07387724585298981\n",
      "Epoch 24/50, Loss: 0.06069942866452038\n",
      "Epoch 25/50, Loss: 0.027501411187196418\n",
      "Epoch 26/50, Loss: 0.08934360050729342\n",
      "Epoch 27/50, Loss: 0.08421159951415445\n",
      "Epoch 28/50, Loss: 0.15521157306752034\n",
      "Epoch 29/50, Loss: 0.1017220988869667\n",
      "Early stopping at epoch 30/50, best loss: 0.027501411187196418\n",
      "Epoch 1/50, Loss: 1.2551872049059187\n",
      "Epoch 2/50, Loss: 1.091274048600878\n",
      "Epoch 3/50, Loss: 0.9603830320494515\n",
      "Epoch 4/50, Loss: 0.8684597781726292\n",
      "Epoch 5/50, Loss: 0.6963572672435215\n",
      "Epoch 6/50, Loss: 0.5931246365819659\n",
      "Epoch 7/50, Loss: 0.5815775224140712\n",
      "Epoch 8/50, Loss: 0.49249941962105886\n",
      "Epoch 9/50, Loss: 0.38758285130773273\n",
      "Epoch 10/50, Loss: 0.34847007053239004\n",
      "Epoch 11/50, Loss: 0.23578739166259766\n",
      "Epoch 12/50, Loss: 0.24396051253591264\n",
      "Epoch 13/50, Loss: 0.35377578330891474\n",
      "Epoch 14/50, Loss: 0.14875684758382185\n",
      "Epoch 15/50, Loss: 0.21052905917167664\n",
      "Epoch 16/50, Loss: 0.21747932849185808\n",
      "Epoch 17/50, Loss: 0.19513518536197288\n",
      "Epoch 18/50, Loss: 0.1243661040706294\n",
      "Epoch 19/50, Loss: 0.07177603590701308\n",
      "Epoch 20/50, Loss: 0.057837866845407655\n",
      "Epoch 21/50, Loss: 0.04480679679129805\n",
      "Epoch 22/50, Loss: 0.022099338671458617\n",
      "Epoch 23/50, Loss: 0.03758023394870439\n",
      "Epoch 24/50, Loss: 0.06830566567701421\n",
      "Epoch 25/50, Loss: 0.038571676039802175\n",
      "Epoch 26/50, Loss: 0.021251930705537752\n",
      "Epoch 27/50, Loss: 0.013621269459170955\n",
      "Epoch 28/50, Loss: 0.022602736766982292\n",
      "Epoch 29/50, Loss: 0.004735077140919332\n",
      "Epoch 30/50, Loss: 0.013340222998522222\n",
      "Epoch 31/50, Loss: 0.0041172898524694445\n",
      "Epoch 32/50, Loss: 0.0009657668581764613\n",
      "Epoch 33/50, Loss: 0.002516802042789225\n",
      "Epoch 34/50, Loss: 0.004876150376779671\n",
      "Epoch 35/50, Loss: 0.0026186651645860237\n",
      "Epoch 36/50, Loss: 0.0008375110711702811\n",
      "Epoch 37/50, Loss: 0.0012312818247924692\n",
      "Epoch 38/50, Loss: 0.001284747026927237\n",
      "Epoch 39/50, Loss: 0.0010244445667402552\n",
      "Epoch 40/50, Loss: 0.0008597166371535943\n",
      "Early stopping at epoch 41/50, best loss: 0.0008375110711702811\n",
      "Epoch 1/50, Loss: 1.229508944920131\n",
      "Epoch 2/50, Loss: 1.1046336378370012\n",
      "Epoch 3/50, Loss: 0.8942537988935199\n",
      "Epoch 4/50, Loss: 0.8099207367215838\n",
      "Epoch 5/50, Loss: 0.7638172890458789\n",
      "Epoch 6/50, Loss: 0.7095247251646859\n",
      "Epoch 7/50, Loss: 0.6339244757379804\n",
      "Epoch 8/50, Loss: 0.42273251126919476\n",
      "Epoch 9/50, Loss: 0.3953832962683269\n",
      "Epoch 10/50, Loss: 0.3222651651927403\n",
      "Epoch 11/50, Loss: 0.2410858358655657\n",
      "Epoch 12/50, Loss: 0.17166692896613053\n",
      "Epoch 13/50, Loss: 0.19710137588637217\n",
      "Epoch 14/50, Loss: 0.31142787635326385\n",
      "Epoch 15/50, Loss: 0.3436486635889326\n",
      "Epoch 16/50, Loss: 0.1510187994156565\n",
      "Epoch 17/50, Loss: 0.15593210554548673\n",
      "Epoch 18/50, Loss: 0.2014999682349818\n",
      "Epoch 19/50, Loss: 0.07664734763758523\n",
      "Epoch 20/50, Loss: 0.09049004169979266\n",
      "Epoch 21/50, Loss: 0.08430678916296788\n",
      "Epoch 22/50, Loss: 0.021104748560381786\n",
      "Epoch 23/50, Loss: 0.21295999815421446\n",
      "Epoch 24/50, Loss: 0.052180263213813305\n",
      "Epoch 25/50, Loss: 0.10155205781172429\n",
      "Epoch 26/50, Loss: 0.04787614482588002\n",
      "Early stopping at epoch 27/50, best loss: 0.021104748560381786\n",
      "Epoch 1/50, Loss: 1.466843775340489\n",
      "Epoch 2/50, Loss: 1.3318571874073573\n",
      "Epoch 3/50, Loss: 1.104626681123461\n",
      "Epoch 4/50, Loss: 1.269524838243212\n",
      "Epoch 5/50, Loss: 1.046817123889923\n",
      "Epoch 6/50, Loss: 1.0365241255078996\n",
      "Epoch 7/50, Loss: 0.9889632548604693\n",
      "Epoch 8/50, Loss: 0.8829515320914132\n",
      "Epoch 9/50, Loss: 0.9735520226614816\n",
      "Epoch 10/50, Loss: 0.9762919460024152\n",
      "Epoch 11/50, Loss: 0.8299685205732074\n",
      "Epoch 12/50, Loss: 0.8867528779166085\n",
      "Epoch 13/50, Loss: 1.043850839138031\n",
      "Epoch 14/50, Loss: 0.8715539148875645\n",
      "Epoch 15/50, Loss: 0.906375527381897\n",
      "Early stopping at epoch 16/50, best loss: 0.8299685205732074\n",
      "Epoch 1/50, Loss: 2.129018613270351\n",
      "Epoch 2/50, Loss: 1.1952342987060547\n",
      "Epoch 3/50, Loss: 1.3196754114968436\n",
      "Epoch 4/50, Loss: 1.152597623211997\n",
      "Epoch 5/50, Loss: 0.9936183946473258\n",
      "Epoch 6/50, Loss: 0.9955218178885323\n",
      "Epoch 7/50, Loss: 0.92716874395098\n",
      "Epoch 8/50, Loss: 0.9309450898851667\n",
      "Epoch 9/50, Loss: 0.9313300422259739\n",
      "Epoch 10/50, Loss: 0.9742160609790257\n",
      "Epoch 11/50, Loss: 0.8700076937675476\n",
      "Epoch 12/50, Loss: 0.7888922435896737\n",
      "Epoch 13/50, Loss: 0.8877295170511518\n",
      "Epoch 14/50, Loss: 0.7777713792664664\n",
      "Epoch 15/50, Loss: 0.8676640135901315\n",
      "Epoch 16/50, Loss: 0.843610976423536\n",
      "Epoch 17/50, Loss: 0.8748026064464024\n",
      "Epoch 18/50, Loss: 1.0204205683299474\n",
      "Early stopping at epoch 19/50, best loss: 0.7777713792664664\n",
      "Epoch 1/50, Loss: 2.3795574392591203\n",
      "Epoch 2/50, Loss: 1.511975850377764\n",
      "Epoch 3/50, Loss: 1.2963220732552665\n",
      "Epoch 4/50, Loss: 1.0996594769614083\n",
      "Epoch 5/50, Loss: 1.2448474849973405\n",
      "Epoch 6/50, Loss: 0.9746877380779811\n",
      "Epoch 7/50, Loss: 1.1012052042143685\n",
      "Epoch 8/50, Loss: 1.0996575525828771\n",
      "Epoch 9/50, Loss: 0.8761361198765891\n",
      "Epoch 10/50, Loss: 0.9279187619686127\n",
      "Epoch 11/50, Loss: 0.8260140504155841\n",
      "Epoch 12/50, Loss: 0.9548731190817696\n",
      "Epoch 13/50, Loss: 1.0538405861173357\n",
      "Epoch 14/50, Loss: 0.9172983425004142\n",
      "Epoch 15/50, Loss: 0.9350433775356838\n",
      "Early stopping at epoch 16/50, best loss: 0.8260140504155841\n",
      "Epoch 1/50, Loss: 1.367053747177124\n",
      "Epoch 2/50, Loss: 1.3209636041096278\n",
      "Epoch 3/50, Loss: 1.2773263113839286\n",
      "Epoch 4/50, Loss: 1.2275890963418143\n",
      "Epoch 5/50, Loss: 1.1673063720975603\n",
      "Epoch 6/50, Loss: 1.1587114674704415\n",
      "Epoch 7/50, Loss: 1.1077809844698225\n",
      "Epoch 8/50, Loss: 1.074511970792498\n",
      "Epoch 9/50, Loss: 1.0544736725943429\n",
      "Epoch 10/50, Loss: 0.9883977259908404\n",
      "Epoch 11/50, Loss: 0.9739808099610465\n",
      "Epoch 12/50, Loss: 0.9416075689452035\n",
      "Epoch 13/50, Loss: 0.910144397190639\n",
      "Epoch 14/50, Loss: 0.8816141486167908\n",
      "Epoch 15/50, Loss: 0.8653320244380406\n",
      "Epoch 16/50, Loss: 0.84274309022086\n",
      "Epoch 17/50, Loss: 0.7745568411690849\n",
      "Epoch 18/50, Loss: 0.8071628127779279\n",
      "Epoch 19/50, Loss: 0.7612665210451398\n",
      "Epoch 20/50, Loss: 0.6549006232193538\n",
      "Epoch 21/50, Loss: 0.6987222858837673\n",
      "Epoch 22/50, Loss: 0.7314544320106506\n",
      "Epoch 23/50, Loss: 0.6221182772091457\n",
      "Epoch 24/50, Loss: 0.6638914091246468\n",
      "Epoch 25/50, Loss: 0.632217845746449\n",
      "Epoch 26/50, Loss: 0.5319046122687203\n",
      "Epoch 27/50, Loss: 0.509534593139376\n",
      "Epoch 28/50, Loss: 0.5184577362877982\n",
      "Epoch 29/50, Loss: 0.570859066077641\n",
      "Epoch 30/50, Loss: 0.5261190746511731\n",
      "Epoch 31/50, Loss: 0.4361024669238499\n",
      "Epoch 32/50, Loss: 0.4492863799844469\n",
      "Epoch 33/50, Loss: 0.3996628906045641\n",
      "Epoch 34/50, Loss: 0.3527419034923826\n",
      "Epoch 35/50, Loss: 0.3532618795122419\n",
      "Epoch 36/50, Loss: 0.3085811457463673\n",
      "Epoch 37/50, Loss: 0.32667251569884165\n",
      "Epoch 38/50, Loss: 0.30320086223738535\n",
      "Epoch 39/50, Loss: 0.27577733780656544\n",
      "Epoch 40/50, Loss: 0.2340328363435609\n",
      "Epoch 41/50, Loss: 0.24038037870611464\n",
      "Epoch 42/50, Loss: 0.24744433803217752\n",
      "Epoch 43/50, Loss: 0.18560860412461416\n",
      "Epoch 44/50, Loss: 0.18305215718490736\n",
      "Epoch 45/50, Loss: 0.2016029187611171\n",
      "Epoch 46/50, Loss: 0.18518166350466864\n",
      "Epoch 47/50, Loss: 0.16569079937679426\n",
      "Epoch 48/50, Loss: 0.12066507233040673\n",
      "Epoch 49/50, Loss: 0.12443979749722141\n",
      "Epoch 50/50, Loss: 0.11013515506471906\n",
      "Epoch 1/50, Loss: 1.362586600439889\n",
      "Epoch 2/50, Loss: 1.3184526307242257\n",
      "Epoch 3/50, Loss: 1.275996378489903\n",
      "Epoch 4/50, Loss: 1.2267916713442122\n",
      "Epoch 5/50, Loss: 1.1777405909129552\n",
      "Epoch 6/50, Loss: 1.1245234778949194\n",
      "Epoch 7/50, Loss: 1.1240150758198328\n",
      "Epoch 8/50, Loss: 1.0612670353480749\n",
      "Epoch 9/50, Loss: 1.005257887499673\n",
      "Epoch 10/50, Loss: 0.9916874851499285\n",
      "Epoch 11/50, Loss: 0.9608460153852191\n",
      "Epoch 12/50, Loss: 0.9181539842060634\n",
      "Epoch 13/50, Loss: 0.9456949830055237\n",
      "Epoch 14/50, Loss: 0.8937827774456569\n",
      "Epoch 15/50, Loss: 0.8645951662744794\n",
      "Epoch 16/50, Loss: 0.8001844201769147\n",
      "Epoch 17/50, Loss: 0.7562200725078583\n",
      "Epoch 18/50, Loss: 0.8315616079739162\n",
      "Epoch 19/50, Loss: 0.7910020010811942\n",
      "Epoch 20/50, Loss: 0.6597134556089129\n",
      "Epoch 21/50, Loss: 0.6652787838663373\n",
      "Epoch 22/50, Loss: 0.6467053805078778\n",
      "Epoch 23/50, Loss: 0.5763747521809169\n",
      "Epoch 24/50, Loss: 0.5625521796090263\n",
      "Epoch 25/50, Loss: 0.5690080055168697\n",
      "Epoch 26/50, Loss: 0.6207328609057835\n",
      "Epoch 27/50, Loss: 0.5657832963126046\n",
      "Epoch 28/50, Loss: 0.46306662048612324\n",
      "Epoch 29/50, Loss: 0.4738694855145046\n",
      "Epoch 30/50, Loss: 0.45097520521708895\n",
      "Epoch 31/50, Loss: 0.4715485955987658\n",
      "Epoch 32/50, Loss: 0.48745512323720114\n",
      "Epoch 33/50, Loss: 0.3891974687576294\n",
      "Epoch 34/50, Loss: 0.5206008383205959\n",
      "Epoch 35/50, Loss: 0.35481128096580505\n",
      "Epoch 36/50, Loss: 0.29063143687588827\n",
      "Epoch 37/50, Loss: 0.341008973973138\n",
      "Epoch 38/50, Loss: 0.28516019880771637\n",
      "Epoch 39/50, Loss: 0.2790530962603433\n",
      "Epoch 40/50, Loss: 0.2346320684467043\n",
      "Epoch 41/50, Loss: 0.2308891671044486\n",
      "Epoch 42/50, Loss: 0.19025622015552862\n",
      "Epoch 43/50, Loss: 0.2086600192955562\n",
      "Epoch 44/50, Loss: 0.2217313370534352\n",
      "Epoch 45/50, Loss: 0.1697468842778887\n",
      "Epoch 46/50, Loss: 0.1562778992312295\n",
      "Epoch 47/50, Loss: 0.1663824985069888\n",
      "Epoch 48/50, Loss: 0.15340634967599595\n",
      "Epoch 49/50, Loss: 0.1556484198995999\n",
      "Epoch 50/50, Loss: 0.1526444357420717\n",
      "Epoch 1/50, Loss: 1.3813246488571167\n",
      "Epoch 2/50, Loss: 1.3313432591302055\n",
      "Epoch 3/50, Loss: 1.2888140508106776\n",
      "Epoch 4/50, Loss: 1.229969058718\n",
      "Epoch 5/50, Loss: 1.1951498474393571\n",
      "Epoch 6/50, Loss: 1.145638448851449\n",
      "Epoch 7/50, Loss: 1.1033549649374825\n",
      "Epoch 8/50, Loss: 1.0765811375209264\n",
      "Epoch 9/50, Loss: 1.0327285528182983\n",
      "Epoch 10/50, Loss: 1.0165449636323112\n",
      "Epoch 11/50, Loss: 0.9984008073806763\n",
      "Epoch 12/50, Loss: 0.9628319484846932\n",
      "Epoch 13/50, Loss: 0.9239204270499093\n",
      "Epoch 14/50, Loss: 0.8873363733291626\n",
      "Epoch 15/50, Loss: 0.8827471733093262\n",
      "Epoch 16/50, Loss: 0.8471831679344177\n",
      "Epoch 17/50, Loss: 0.8348719222205025\n",
      "Epoch 18/50, Loss: 0.8402138437543597\n",
      "Epoch 19/50, Loss: 0.7624906812395368\n",
      "Epoch 20/50, Loss: 0.7239557121481214\n",
      "Epoch 21/50, Loss: 0.6791243297713143\n",
      "Epoch 22/50, Loss: 0.7371978419167655\n",
      "Epoch 23/50, Loss: 0.6659443378448486\n",
      "Epoch 24/50, Loss: 0.6952786445617676\n",
      "Epoch 25/50, Loss: 0.694430376802172\n",
      "Epoch 26/50, Loss: 0.6008284730570657\n",
      "Epoch 27/50, Loss: 0.6403675547667912\n",
      "Epoch 28/50, Loss: 0.5489644706249237\n",
      "Epoch 29/50, Loss: 0.5769041223185403\n",
      "Epoch 30/50, Loss: 0.5633480080536434\n",
      "Epoch 31/50, Loss: 0.48831083944865633\n",
      "Epoch 32/50, Loss: 0.4927788163934435\n",
      "Epoch 33/50, Loss: 0.4252208173274994\n",
      "Epoch 34/50, Loss: 0.4346786141395569\n",
      "Epoch 35/50, Loss: 0.4374848859650748\n",
      "Epoch 36/50, Loss: 0.383254485470908\n",
      "Epoch 37/50, Loss: 0.3357502264635904\n",
      "Epoch 38/50, Loss: 0.30791785035814556\n",
      "Epoch 39/50, Loss: 0.26712778210639954\n",
      "Epoch 40/50, Loss: 0.3449614133153643\n",
      "Epoch 41/50, Loss: 0.31144792267254423\n",
      "Epoch 42/50, Loss: 0.2758770287036896\n",
      "Epoch 43/50, Loss: 0.32253811401980265\n",
      "Early stopping at epoch 44/50, best loss: 0.26712778210639954\n",
      "Epoch 1/50, Loss: 1.2604536839893885\n",
      "Epoch 2/50, Loss: 1.0189013395990645\n",
      "Epoch 3/50, Loss: 0.9877095477921622\n",
      "Epoch 4/50, Loss: 0.8151866878782\n",
      "Epoch 5/50, Loss: 0.73001286813191\n",
      "Epoch 6/50, Loss: 0.6792406439781189\n",
      "Epoch 7/50, Loss: 0.7008670738765171\n",
      "Epoch 8/50, Loss: 0.5963922228131976\n",
      "Epoch 9/50, Loss: 0.46780142188072205\n",
      "Epoch 10/50, Loss: 0.2989516407251358\n",
      "Epoch 11/50, Loss: 0.3738033516066415\n",
      "Epoch 12/50, Loss: 0.22374317156417028\n",
      "Epoch 13/50, Loss: 0.16318314841815404\n",
      "Epoch 14/50, Loss: 0.1542417850910819\n",
      "Epoch 15/50, Loss: 0.13117828858750208\n",
      "Epoch 16/50, Loss: 0.06721869935946805\n",
      "Epoch 17/50, Loss: 0.06952943267034632\n",
      "Epoch 18/50, Loss: 0.04276190172614796\n",
      "Epoch 19/50, Loss: 0.14495887221502407\n",
      "Epoch 20/50, Loss: 0.10932858833777052\n",
      "Epoch 21/50, Loss: 0.2074516106929098\n",
      "Epoch 22/50, Loss: 0.2591485953172586\n",
      "Early stopping at epoch 23/50, best loss: 0.04276190172614796\n",
      "Epoch 1/50, Loss: 1.1953329358782088\n",
      "Epoch 2/50, Loss: 1.0961579424994332\n",
      "Epoch 3/50, Loss: 0.966333440371922\n",
      "Epoch 4/50, Loss: 0.8034550292151315\n",
      "Epoch 5/50, Loss: 0.6623080968856812\n",
      "Epoch 6/50, Loss: 0.588613931621824\n",
      "Epoch 7/50, Loss: 0.5316788937364306\n",
      "Epoch 8/50, Loss: 0.33760062232613564\n",
      "Epoch 9/50, Loss: 0.35816047872815815\n",
      "Epoch 10/50, Loss: 0.2504589131900242\n",
      "Epoch 11/50, Loss: 0.22391067977462495\n",
      "Epoch 12/50, Loss: 0.21241763340575354\n",
      "Epoch 13/50, Loss: 0.2967758019055639\n",
      "Epoch 14/50, Loss: 0.12022270794425692\n",
      "Epoch 15/50, Loss: 0.11195027349250657\n",
      "Epoch 16/50, Loss: 0.11374624712126595\n",
      "Epoch 17/50, Loss: 0.1790633081857647\n",
      "Epoch 18/50, Loss: 0.27984119951725006\n",
      "Epoch 19/50, Loss: 0.11114770697271784\n",
      "Epoch 20/50, Loss: 0.11234748629587037\n",
      "Epoch 21/50, Loss: 0.09415597940928169\n",
      "Epoch 22/50, Loss: 0.1408803816884756\n",
      "Epoch 23/50, Loss: 0.1899550716791834\n",
      "Epoch 24/50, Loss: 0.08553939931360739\n",
      "Epoch 25/50, Loss: 0.15461890319628374\n",
      "Epoch 26/50, Loss: 0.39612062222191263\n",
      "Epoch 27/50, Loss: 0.15139180417671533\n",
      "Epoch 28/50, Loss: 0.03790791632075395\n",
      "Epoch 29/50, Loss: 0.022789299355021546\n",
      "Epoch 30/50, Loss: 0.03424886241555214\n",
      "Epoch 31/50, Loss: 0.013608180818014912\n",
      "Epoch 32/50, Loss: 0.008704743348062038\n",
      "Epoch 33/50, Loss: 0.024581674250709642\n",
      "Epoch 34/50, Loss: 0.009055701433680952\n",
      "Epoch 35/50, Loss: 0.008512269106826611\n",
      "Epoch 36/50, Loss: 0.008774489213075347\n",
      "Epoch 37/50, Loss: 0.018473602850073285\n",
      "Epoch 38/50, Loss: 0.006610738961691303\n",
      "Epoch 39/50, Loss: 0.09965515604043114\n",
      "Epoch 40/50, Loss: 0.2815397416000321\n",
      "Epoch 41/50, Loss: 0.322152580799801\n",
      "Epoch 42/50, Loss: 0.19951689419602708\n",
      "Early stopping at epoch 43/50, best loss: 0.006610738961691303\n",
      "Epoch 1/50, Loss: 1.197385277066912\n",
      "Epoch 2/50, Loss: 1.0854899883270264\n",
      "Epoch 3/50, Loss: 0.9034302404948643\n",
      "Epoch 4/50, Loss: 0.8284427012716021\n",
      "Epoch 5/50, Loss: 0.761728184563773\n",
      "Epoch 6/50, Loss: 0.6813043015343803\n",
      "Epoch 7/50, Loss: 0.5735149809292385\n",
      "Epoch 8/50, Loss: 0.4840070626565388\n",
      "Epoch 9/50, Loss: 0.3401688258163631\n",
      "Epoch 10/50, Loss: 0.2716498098203114\n",
      "Epoch 11/50, Loss: 0.1952329703739711\n",
      "Epoch 12/50, Loss: 0.24030410604817526\n",
      "Epoch 13/50, Loss: 0.1444929444364139\n",
      "Epoch 14/50, Loss: 0.2004459666620408\n",
      "Epoch 15/50, Loss: 0.12926542652504786\n",
      "Epoch 16/50, Loss: 0.15121016691305808\n",
      "Epoch 17/50, Loss: 0.13237857978258813\n",
      "Epoch 18/50, Loss: 0.055872129542487006\n",
      "Epoch 19/50, Loss: 0.11289114238960403\n",
      "Epoch 20/50, Loss: 0.1100516511526491\n",
      "Epoch 21/50, Loss: 0.14534332995702112\n",
      "Epoch 22/50, Loss: 0.049188548482821455\n",
      "Epoch 23/50, Loss: 0.06797950364333312\n",
      "Epoch 24/50, Loss: 0.04589880537241697\n",
      "Epoch 25/50, Loss: 0.012011630597823699\n",
      "Epoch 26/50, Loss: 0.006019737816781604\n",
      "Epoch 27/50, Loss: 0.02274727262556553\n",
      "Epoch 28/50, Loss: 0.028666495852771083\n",
      "Epoch 29/50, Loss: 0.1089627766819571\n",
      "Epoch 30/50, Loss: 0.05553039702187691\n",
      "Early stopping at epoch 31/50, best loss: 0.006019737816781604\n",
      "Epoch 1/50, Loss: 3.963762743132455\n",
      "Epoch 2/50, Loss: 1.2764053515025549\n",
      "Epoch 3/50, Loss: 1.4748772638184684\n",
      "Epoch 4/50, Loss: 1.0317480393818446\n",
      "Epoch 5/50, Loss: 1.1579752649579729\n",
      "Epoch 6/50, Loss: 1.0015188114983695\n",
      "Epoch 7/50, Loss: 1.1172019583838326\n",
      "Epoch 8/50, Loss: 1.0815575548580714\n",
      "Epoch 9/50, Loss: 1.0460949284689767\n",
      "Epoch 10/50, Loss: 1.0679701992443629\n",
      "Early stopping at epoch 11/50, best loss: 1.0015188114983695\n",
      "Epoch 1/50, Loss: 3.0703968490873064\n",
      "Epoch 2/50, Loss: 1.1735802803720747\n",
      "Epoch 3/50, Loss: 1.3697379061153956\n",
      "Epoch 4/50, Loss: 1.423863342830113\n",
      "Epoch 5/50, Loss: 1.2041000468390328\n",
      "Epoch 6/50, Loss: 1.0992292080606734\n",
      "Epoch 7/50, Loss: 0.9555876680782863\n",
      "Epoch 8/50, Loss: 0.9333179593086243\n",
      "Epoch 9/50, Loss: 0.955652015549796\n",
      "Epoch 10/50, Loss: 1.0586606689861842\n",
      "Epoch 11/50, Loss: 0.951011427811214\n",
      "Epoch 12/50, Loss: 0.8836300713675362\n",
      "Epoch 13/50, Loss: 1.0708271179880415\n",
      "Epoch 14/50, Loss: 0.9300729717527118\n",
      "Epoch 15/50, Loss: 1.0955531001091003\n",
      "Epoch 16/50, Loss: 1.1035451974187578\n",
      "Early stopping at epoch 17/50, best loss: 0.8836300713675362\n",
      "Epoch 1/50, Loss: 2.255810592855726\n",
      "Epoch 2/50, Loss: 1.3846115214484078\n",
      "Epoch 3/50, Loss: 1.1397549084254675\n",
      "Epoch 4/50, Loss: 1.1130242007119315\n",
      "Epoch 5/50, Loss: 1.2385031751223974\n",
      "Epoch 6/50, Loss: 1.090185182435172\n",
      "Epoch 7/50, Loss: 1.1290305171694075\n",
      "Epoch 8/50, Loss: 1.0990246449198042\n",
      "Epoch 9/50, Loss: 0.9657741614750454\n",
      "Epoch 10/50, Loss: 0.9376896279198783\n",
      "Epoch 11/50, Loss: 1.1784581712314062\n",
      "Epoch 12/50, Loss: 1.4278915524482727\n",
      "Epoch 13/50, Loss: 0.9395506296839032\n",
      "Epoch 14/50, Loss: 1.1259922470365251\n",
      "Early stopping at epoch 15/50, best loss: 0.9376896279198783\n",
      "Epoch 1/50, Loss: 1.3663698434829712\n",
      "Epoch 2/50, Loss: 1.3150316306522913\n",
      "Epoch 3/50, Loss: 1.2489947761808122\n",
      "Epoch 4/50, Loss: 1.192138705934797\n",
      "Epoch 5/50, Loss: 1.1257787772587367\n",
      "Epoch 6/50, Loss: 1.0739357130868095\n",
      "Epoch 7/50, Loss: 1.0620773945535933\n",
      "Epoch 8/50, Loss: 0.9973407643181937\n",
      "Epoch 9/50, Loss: 0.9413997105189732\n",
      "Epoch 10/50, Loss: 0.9361441731452942\n",
      "Epoch 11/50, Loss: 0.8774940201214382\n",
      "Epoch 12/50, Loss: 0.8304906913212368\n",
      "Epoch 13/50, Loss: 0.8419441751071385\n",
      "Epoch 14/50, Loss: 0.8125473601477486\n",
      "Epoch 15/50, Loss: 0.7665843197277614\n",
      "Epoch 16/50, Loss: 0.7357037237712315\n",
      "Epoch 17/50, Loss: 0.698973319360188\n",
      "Epoch 18/50, Loss: 0.7334371379443577\n",
      "Epoch 19/50, Loss: 0.6139619946479797\n",
      "Epoch 20/50, Loss: 0.5860609837940761\n",
      "Epoch 21/50, Loss: 0.5897840516907829\n",
      "Epoch 22/50, Loss: 0.6190013289451599\n",
      "Epoch 23/50, Loss: 0.5250344617026192\n",
      "Epoch 24/50, Loss: 0.5694320159299033\n",
      "Epoch 25/50, Loss: 0.44581741094589233\n",
      "Epoch 26/50, Loss: 0.4513322753565652\n",
      "Epoch 27/50, Loss: 0.4126436582633427\n",
      "Epoch 28/50, Loss: 0.37516786796706064\n",
      "Epoch 29/50, Loss: 0.37286307769162313\n",
      "Epoch 30/50, Loss: 0.3237768007176263\n",
      "Epoch 31/50, Loss: 0.3068180595125471\n",
      "Epoch 32/50, Loss: 0.3009009999888284\n",
      "Epoch 33/50, Loss: 0.2378439541373934\n",
      "Epoch 34/50, Loss: 0.26005845836230684\n",
      "Epoch 35/50, Loss: 0.20579482508557184\n",
      "Epoch 36/50, Loss: 0.1827597634068557\n",
      "Epoch 37/50, Loss: 0.17985200456210546\n",
      "Epoch 38/50, Loss: 0.1614837489489998\n",
      "Epoch 39/50, Loss: 0.17633741348981857\n",
      "Epoch 40/50, Loss: 0.2521169771041189\n",
      "Epoch 41/50, Loss: 0.1317476204463414\n",
      "Epoch 42/50, Loss: 0.161949808044093\n",
      "Epoch 43/50, Loss: 0.09246834327599832\n",
      "Epoch 44/50, Loss: 0.0912397453295333\n",
      "Epoch 45/50, Loss: 0.11424880740897995\n",
      "Epoch 46/50, Loss: 0.092044219906841\n",
      "Epoch 47/50, Loss: 0.06598304584622383\n",
      "Epoch 48/50, Loss: 0.07218188898903984\n",
      "Epoch 49/50, Loss: 0.08628294244408607\n",
      "Epoch 50/50, Loss: 0.06515667587518692\n",
      "Epoch 1/50, Loss: 1.3602225950786047\n",
      "Epoch 2/50, Loss: 1.3137955154691423\n",
      "Epoch 3/50, Loss: 1.2576720033373152\n",
      "Epoch 4/50, Loss: 1.1974266597202845\n",
      "Epoch 5/50, Loss: 1.1451849937438965\n",
      "Epoch 6/50, Loss: 1.0887786660875594\n",
      "Epoch 7/50, Loss: 1.0819083111626762\n",
      "Epoch 8/50, Loss: 1.0319892678942\n",
      "Epoch 9/50, Loss: 1.00913006067276\n",
      "Epoch 10/50, Loss: 0.9662435735974994\n",
      "Epoch 11/50, Loss: 0.9491219350269863\n",
      "Epoch 12/50, Loss: 0.9347379037312099\n",
      "Epoch 13/50, Loss: 0.8539480141230992\n",
      "Epoch 14/50, Loss: 0.8182652933256966\n",
      "Epoch 15/50, Loss: 0.7606082814080375\n",
      "Epoch 16/50, Loss: 0.7817624892507281\n",
      "Epoch 17/50, Loss: 0.7117697937147958\n",
      "Epoch 18/50, Loss: 0.767637278352465\n",
      "Epoch 19/50, Loss: 0.6810824530465263\n",
      "Epoch 20/50, Loss: 0.6958056816032955\n",
      "Epoch 21/50, Loss: 0.6034241531576429\n",
      "Epoch 22/50, Loss: 0.5878138542175293\n",
      "Epoch 23/50, Loss: 0.58017127428736\n",
      "Epoch 24/50, Loss: 0.5345123921121869\n",
      "Epoch 25/50, Loss: 0.5062217158930642\n",
      "Epoch 26/50, Loss: 0.49767652579716276\n",
      "Epoch 27/50, Loss: 0.4937779818262373\n",
      "Epoch 28/50, Loss: 0.44001822812216623\n",
      "Epoch 29/50, Loss: 0.38101695690836224\n",
      "Epoch 30/50, Loss: 0.3901267264570509\n",
      "Epoch 31/50, Loss: 0.37550894703183857\n",
      "Epoch 32/50, Loss: 0.3791561552456447\n",
      "Epoch 33/50, Loss: 0.281820294048105\n",
      "Epoch 34/50, Loss: 0.398195600935391\n",
      "Epoch 35/50, Loss: 0.29994905633585794\n",
      "Epoch 36/50, Loss: 0.25738600535052164\n",
      "Epoch 37/50, Loss: 0.2241401353052684\n",
      "Epoch 38/50, Loss: 0.22517063362257822\n",
      "Epoch 39/50, Loss: 0.25113959184714724\n",
      "Epoch 40/50, Loss: 0.2199612557888031\n",
      "Epoch 41/50, Loss: 0.16656119376420975\n",
      "Epoch 42/50, Loss: 0.1896380569253649\n",
      "Epoch 43/50, Loss: 0.13108548096248082\n",
      "Epoch 44/50, Loss: 0.13589150884321757\n",
      "Epoch 45/50, Loss: 0.11341299329485212\n",
      "Epoch 46/50, Loss: 0.11885974343333926\n",
      "Epoch 47/50, Loss: 0.08414002720798765\n",
      "Epoch 48/50, Loss: 0.07883398767028536\n",
      "Epoch 49/50, Loss: 0.08725716971925326\n",
      "Epoch 50/50, Loss: 0.0680884521986757\n",
      "Epoch 1/50, Loss: 1.3617911849703108\n",
      "Epoch 2/50, Loss: 1.3049378905977522\n",
      "Epoch 3/50, Loss: 1.2512802907398768\n",
      "Epoch 4/50, Loss: 1.2009004013878959\n",
      "Epoch 5/50, Loss: 1.138101305280413\n",
      "Epoch 6/50, Loss: 1.1018736532756261\n",
      "Epoch 7/50, Loss: 1.0518354858670915\n",
      "Epoch 8/50, Loss: 1.000324479171208\n",
      "Epoch 9/50, Loss: 0.9566238692828587\n",
      "Epoch 10/50, Loss: 0.9181023154939923\n",
      "Epoch 11/50, Loss: 0.8907592722347805\n",
      "Epoch 12/50, Loss: 0.8885624238422939\n",
      "Epoch 13/50, Loss: 0.8276399118559701\n",
      "Epoch 14/50, Loss: 0.8572091545377459\n",
      "Epoch 15/50, Loss: 0.7464926626001086\n",
      "Epoch 16/50, Loss: 0.7148833743163517\n",
      "Epoch 17/50, Loss: 0.7053699493408203\n",
      "Epoch 18/50, Loss: 0.6535029496465411\n",
      "Epoch 19/50, Loss: 0.6549112115587506\n",
      "Epoch 20/50, Loss: 0.6333658567496708\n",
      "Epoch 21/50, Loss: 0.5973548420837947\n",
      "Epoch 22/50, Loss: 0.5496331708771842\n",
      "Epoch 23/50, Loss: 0.6218589672020504\n",
      "Epoch 24/50, Loss: 0.5072053585733686\n",
      "Epoch 25/50, Loss: 0.42605261717523846\n",
      "Epoch 26/50, Loss: 0.5148628056049347\n",
      "Epoch 27/50, Loss: 0.42199476276125225\n",
      "Epoch 28/50, Loss: 0.3797898164817265\n",
      "Epoch 29/50, Loss: 0.37887564301490784\n",
      "Epoch 30/50, Loss: 0.40123158267566134\n",
      "Epoch 31/50, Loss: 0.35672090096133097\n",
      "Epoch 32/50, Loss: 0.35568997051034656\n",
      "Epoch 33/50, Loss: 0.26146350481680464\n",
      "Epoch 34/50, Loss: 0.30356388219765257\n",
      "Epoch 35/50, Loss: 0.29203714430332184\n",
      "Epoch 36/50, Loss: 0.2079063675233296\n",
      "Epoch 37/50, Loss: 0.2166481614112854\n",
      "Epoch 38/50, Loss: 0.17731580138206482\n",
      "Epoch 39/50, Loss: 0.16326741661344255\n",
      "Epoch 40/50, Loss: 0.1799956719790186\n",
      "Epoch 41/50, Loss: 0.1444356462785176\n",
      "Epoch 42/50, Loss: 0.14560769817658833\n",
      "Epoch 43/50, Loss: 0.13356937680925643\n",
      "Epoch 44/50, Loss: 0.1318903331245695\n",
      "Epoch 45/50, Loss: 0.12030433863401413\n",
      "Epoch 46/50, Loss: 0.10799611892019\n",
      "Epoch 47/50, Loss: 0.09232866604413305\n",
      "Epoch 48/50, Loss: 0.07557113455342394\n",
      "Epoch 49/50, Loss: 0.07638119826359409\n",
      "Epoch 50/50, Loss: 0.060603804886341095\n",
      "Epoch 1/50, Loss: 1.2431986842836653\n",
      "Epoch 2/50, Loss: 1.0648911425045557\n",
      "Epoch 3/50, Loss: 1.019571099962507\n",
      "Epoch 4/50, Loss: 0.8140214851924351\n",
      "Epoch 5/50, Loss: 0.6629210965973991\n",
      "Epoch 6/50, Loss: 0.516914427280426\n",
      "Epoch 7/50, Loss: 0.5315893079553332\n",
      "Epoch 8/50, Loss: 0.42662184153284344\n",
      "Epoch 9/50, Loss: 0.3423096665314266\n",
      "Epoch 10/50, Loss: 0.25426037183829714\n",
      "Epoch 11/50, Loss: 0.26844809842961176\n",
      "Epoch 12/50, Loss: 0.21283965877124242\n",
      "Epoch 13/50, Loss: 0.1532112221632685\n",
      "Epoch 14/50, Loss: 0.17022025319082396\n",
      "Epoch 15/50, Loss: 0.17109005259616034\n",
      "Epoch 16/50, Loss: 0.12555625728730643\n",
      "Epoch 17/50, Loss: 0.04915430450013706\n",
      "Epoch 18/50, Loss: 0.05158939112776092\n",
      "Epoch 19/50, Loss: 0.05557665787637234\n",
      "Epoch 20/50, Loss: 0.05247159632355241\n",
      "Epoch 21/50, Loss: 0.019894086115527898\n",
      "Epoch 22/50, Loss: 0.01260626657001142\n",
      "Epoch 23/50, Loss: 0.008318354916160129\n",
      "Epoch 24/50, Loss: 0.007929136510938406\n",
      "Epoch 25/50, Loss: 0.012127309522059346\n",
      "Epoch 26/50, Loss: 0.022594412917637134\n",
      "Epoch 27/50, Loss: 0.025603074813261628\n",
      "Epoch 28/50, Loss: 0.06351656430550585\n",
      "Early stopping at epoch 29/50, best loss: 0.007929136510938406\n",
      "Epoch 1/50, Loss: 1.1642426763262068\n",
      "Epoch 2/50, Loss: 1.0103730985096522\n",
      "Epoch 3/50, Loss: 0.8700319698878697\n",
      "Epoch 4/50, Loss: 0.8531373994691032\n",
      "Epoch 5/50, Loss: 0.6712562016078404\n",
      "Epoch 6/50, Loss: 0.7036305112498147\n",
      "Epoch 7/50, Loss: 0.5459413698741368\n",
      "Epoch 8/50, Loss: 0.3935235547167914\n",
      "Epoch 9/50, Loss: 0.3638714381626674\n",
      "Epoch 10/50, Loss: 0.21578568007264817\n",
      "Epoch 11/50, Loss: 0.1686320581606456\n",
      "Epoch 12/50, Loss: 0.17303699254989624\n",
      "Epoch 13/50, Loss: 0.1377509246979441\n",
      "Epoch 14/50, Loss: 0.12847233430615493\n",
      "Epoch 15/50, Loss: 0.12199636229446956\n",
      "Epoch 16/50, Loss: 0.1634931046116565\n",
      "Epoch 17/50, Loss: 0.09178685548249632\n",
      "Epoch 18/50, Loss: 0.04751294545297112\n",
      "Epoch 19/50, Loss: 0.06195176685495036\n",
      "Epoch 20/50, Loss: 0.038898083909381445\n",
      "Epoch 21/50, Loss: 0.0134419061110488\n",
      "Epoch 22/50, Loss: 0.009340074478781648\n",
      "Epoch 23/50, Loss: 0.01588383548161281\n",
      "Epoch 24/50, Loss: 0.012365405181688922\n",
      "Epoch 25/50, Loss: 0.08947904633948513\n",
      "Epoch 26/50, Loss: 0.030332428363700665\n",
      "Early stopping at epoch 27/50, best loss: 0.009340074478781648\n",
      "Epoch 1/50, Loss: 1.2193304470607214\n",
      "Epoch 2/50, Loss: 1.0821641342980521\n",
      "Epoch 3/50, Loss: 0.962147695677621\n",
      "Epoch 4/50, Loss: 0.8078925950186593\n",
      "Epoch 5/50, Loss: 0.6562697546822684\n",
      "Epoch 6/50, Loss: 0.6206075463976178\n",
      "Epoch 7/50, Loss: 0.4622622323887689\n",
      "Epoch 8/50, Loss: 0.4360900436128889\n",
      "Epoch 9/50, Loss: 0.31733592067446026\n",
      "Epoch 10/50, Loss: 0.201192778136049\n",
      "Epoch 11/50, Loss: 0.17048692171062743\n",
      "Epoch 12/50, Loss: 0.13924898047532355\n",
      "Epoch 13/50, Loss: 0.09858756246311325\n",
      "Epoch 14/50, Loss: 0.06003408879041672\n",
      "Epoch 15/50, Loss: 0.059861327920641215\n",
      "Epoch 16/50, Loss: 0.034235676330614036\n",
      "Epoch 17/50, Loss: 0.058434975815803876\n",
      "Epoch 18/50, Loss: 0.0853256383644683\n",
      "Epoch 19/50, Loss: 0.06471691480172533\n",
      "Epoch 20/50, Loss: 0.04278376302681863\n",
      "Early stopping at epoch 21/50, best loss: 0.034235676330614036\n",
      "Epoch 1/50, Loss: 3.199119976588658\n",
      "Epoch 2/50, Loss: 1.3353410618645805\n",
      "Epoch 3/50, Loss: 1.506972142628261\n",
      "Epoch 4/50, Loss: 1.140710439000811\n",
      "Epoch 5/50, Loss: 1.1462841544832503\n",
      "Epoch 6/50, Loss: 1.09750269140516\n",
      "Epoch 7/50, Loss: 1.1131840263094221\n",
      "Epoch 8/50, Loss: 1.078966702733721\n",
      "Epoch 9/50, Loss: 1.0498036657060896\n",
      "Epoch 10/50, Loss: 1.0858285597392492\n",
      "Epoch 11/50, Loss: 1.1012798377445765\n",
      "Epoch 12/50, Loss: 1.0525590181350708\n",
      "Epoch 13/50, Loss: 1.068789882319314\n",
      "Epoch 14/50, Loss: 1.0398629988942827\n",
      "Epoch 15/50, Loss: 1.1210359164646693\n",
      "Epoch 16/50, Loss: 1.053379169532231\n",
      "Epoch 17/50, Loss: 1.0621270963123866\n",
      "Epoch 18/50, Loss: 1.0447608487946647\n",
      "Early stopping at epoch 19/50, best loss: 1.0398629988942827\n",
      "Epoch 1/50, Loss: 3.8327309233801707\n",
      "Epoch 2/50, Loss: 1.648216792515346\n",
      "Epoch 3/50, Loss: 1.39951924766813\n",
      "Epoch 4/50, Loss: 1.1764652558735438\n",
      "Epoch 5/50, Loss: 1.1195245385169983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_615/4158225697.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  specificity = (np.sum(cm) - np.sum(cm, axis = 0) - np.sum(cm, axis = 1) + np.diag(cm)) / (np.sum(cm) - np.sum(cm, axis = 0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/50, Loss: 1.0870654668126787\n",
      "Epoch 7/50, Loss: 1.0764166968209403\n",
      "Epoch 8/50, Loss: 1.100658995764596\n",
      "Epoch 9/50, Loss: 1.4247029764311654\n",
      "Epoch 10/50, Loss: 1.0745293242590768\n",
      "Epoch 11/50, Loss: 1.1438286985669817\n",
      "Epoch 12/50, Loss: 1.112262521471296\n",
      "Epoch 13/50, Loss: 1.094802669116429\n",
      "Epoch 14/50, Loss: 1.1010198933737618\n",
      "Early stopping at epoch 15/50, best loss: 1.0745293242590768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_615/4158225697.py:33: RuntimeWarning: invalid value encountered in divide\n",
      "  specificity = (np.sum(cm) - np.sum(cm, axis = 0) - np.sum(cm, axis = 1) + np.diag(cm)) / (np.sum(cm) - np.sum(cm, axis = 0))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 2.5364006076540266\n",
      "Epoch 2/50, Loss: 1.777518289429801\n",
      "Epoch 3/50, Loss: 1.357111896787371\n",
      "Epoch 4/50, Loss: 1.4341205188206263\n",
      "Epoch 5/50, Loss: 1.2256047895976476\n",
      "Epoch 6/50, Loss: 1.2953356845038277\n",
      "Epoch 7/50, Loss: 1.0882683311189925\n",
      "Epoch 8/50, Loss: 0.9882844090461731\n",
      "Epoch 9/50, Loss: 1.1003705518586295\n",
      "Epoch 10/50, Loss: 1.0888277207102095\n",
      "Epoch 11/50, Loss: 1.026561668940953\n",
      "Epoch 12/50, Loss: 1.1490522537912642\n",
      "Early stopping at epoch 13/50, best loss: 0.9882844090461731\n",
      "  epoch  epoch_loss  best_loss hidden_size  learning_rate weight_decay  \\\n",
      "0    49    0.205147   0.205147         460         0.0001            0   \n",
      "0    49    0.269517   0.231874         460         0.0001      0.00001   \n",
      "0    49    0.225831   0.221439         460         0.0001       0.0001   \n",
      "0    29    0.074579   0.027501         460         0.0010            0   \n",
      "0    40    0.001394   0.000838         460         0.0010      0.00001   \n",
      "0    26    0.030852   0.021105         460         0.0010       0.0001   \n",
      "0    15    0.917315   0.829969         460         0.0100            0   \n",
      "0    18    0.934821   0.777771         460         0.0100      0.00001   \n",
      "0    15    1.090012   0.826014         460         0.0100       0.0001   \n",
      "0    49    0.110135   0.110135         575         0.0001            0   \n",
      "0    49    0.152644   0.152644         575         0.0001      0.00001   \n",
      "0    43    0.271777   0.267128         575         0.0001       0.0001   \n",
      "0    22    0.221727   0.042762         575         0.0010            0   \n",
      "0    42    0.172360   0.006611         575         0.0010      0.00001   \n",
      "0    30    0.159257   0.006020         575         0.0010       0.0001   \n",
      "0    10    1.035649   1.001519         575         0.0100            0   \n",
      "0    16    1.170959   0.883630         575         0.0100      0.00001   \n",
      "0    14    1.149465   0.937690         575         0.0100       0.0001   \n",
      "0    49    0.065157   0.065157         690         0.0001            0   \n",
      "0    49    0.068088   0.068088         690         0.0001      0.00001   \n",
      "0    49    0.060604   0.060604         690         0.0001       0.0001   \n",
      "0    28    0.105441   0.007929         690         0.0010            0   \n",
      "0    26    0.032074   0.009340         690         0.0010      0.00001   \n",
      "0    20    0.091267   0.034236         690         0.0010       0.0001   \n",
      "0    18    1.072342   1.039863         690         0.0100            0   \n",
      "0    14    1.116280   1.074529         690         0.0100      0.00001   \n",
      "0    12    1.066082   0.988284         690         0.0100       0.0001   \n",
      "\n",
      "   accuracy                          sensitivity  \\\n",
      "0  0.822581    [0.9, 0.8125, 0.7692307692307693]   \n",
      "0  0.758065   [0.95, 0.8125, 0.5769230769230769]   \n",
      "0  0.806452    [0.9, 0.8125, 0.7307692307692307]   \n",
      "0  0.838710   [0.95, 0.8125, 0.7692307692307693]   \n",
      "0  0.854839    [0.95, 0.875, 0.7692307692307693]   \n",
      "0  0.790323    [0.8, 0.8125, 0.7692307692307693]   \n",
      "0  0.596774   [0.75, 0.625, 0.46153846153846156]   \n",
      "0  0.677419    [0.4, 0.6875, 0.8846153846153846]   \n",
      "0  0.580645                   [0.8, 0.4375, 0.5]   \n",
      "0  0.790323    [0.9, 0.8125, 0.6923076923076923]   \n",
      "0  0.758065   [0.85, 0.8125, 0.6538461538461539]   \n",
      "0  0.725806      [0.9, 0.75, 0.5769230769230769]   \n",
      "0  0.758065    [0.95, 0.875, 0.5384615384615384]   \n",
      "0  0.822581   [0.85, 0.8125, 0.8076923076923077]   \n",
      "0  0.822581   [0.95, 0.8125, 0.7307692307692307]   \n",
      "0  0.467742      [0.25, 0.0, 0.9230769230769231]   \n",
      "0  0.483871   [0.15, 0.6875, 0.6153846153846154]   \n",
      "0  0.516129  [0.35, 0.8125, 0.46153846153846156]   \n",
      "0  0.806452   [0.95, 0.8125, 0.6923076923076923]   \n",
      "0  0.790323    [0.9, 0.8125, 0.6923076923076923]   \n",
      "0  0.838710   [0.95, 0.8125, 0.7692307692307693]   \n",
      "0  0.870968    [0.85, 0.875, 0.8846153846153846]   \n",
      "0  0.854839    [0.9, 0.8125, 0.8461538461538461]   \n",
      "0  0.838710   [0.95, 0.8125, 0.7692307692307693]   \n",
      "0  0.419355                      [0.0, 0.0, 1.0]   \n",
      "0  0.419355                      [0.0, 0.0, 1.0]   \n",
      "0  0.483871     [0.0, 0.375, 0.9230769230769231]   \n",
      "\n",
      "                                         specificity  \n",
      "0   [0.9473684210526315, 0.9375, 0.8421052631578947]  \n",
      "0     [0.9705882352941176, 0.9347826086956522, 0.75]  \n",
      "0   [0.9459459459459459, 0.9375, 0.8205128205128205]  \n",
      "0    [0.972972972972973, 0.9375, 0.8461538461538461]  \n",
      "0      [0.972972972972973, 0.9574468085106383, 0.85]  \n",
      "0  [0.9024390243902439, 0.9333333333333333, 0.842...  \n",
      "0  [0.8333333333333334, 0.8823529411764706, 0.674...  \n",
      "0  [0.7692307692307693, 0.8888888888888888, 0.888...  \n",
      "0  [0.8620689655172413, 0.8235294117647058, 0.704...  \n",
      "0      [0.9459459459459459, 0.9361702127659575, 0.8]  \n",
      "0     [0.918918918918919, 0.9361702127659575, 0.775]  \n",
      "0                 [0.9375, 0.9166666666666666, 0.75]  \n",
      "0  [0.96875, 0.9565217391304348, 0.7391304347826086]  \n",
      "0  [0.926829268292683, 0.9347826086956522, 0.8648...  \n",
      "0     [0.972972972972973, 0.9361702127659575, 0.825]  \n",
      "0  [0.7169811320754716, 0.7419354838709677, 0.777...  \n",
      "0   [0.7068965517241379, 0.8529411764705882, 0.6875]  \n",
      "0                    [0.75, 0.9, 0.6666666666666666]  \n",
      "0  [0.972972972972973, 0.9347826086956522, 0.8048...  \n",
      "0      [0.9459459459459459, 0.9361702127659575, 0.8]  \n",
      "0  [0.9743589743589743, 0.9347826086956522, 0.846...  \n",
      "0  [0.9302325581395349, 0.9555555555555556, 0.916...  \n",
      "0  [0.9512195121951219, 0.9347826086956522, 0.891...  \n",
      "0  [0.9736842105263158, 0.9361702127659575, 0.846...  \n",
      "0      [0.6774193548387096, 0.7419354838709677, nan]  \n",
      "0      [0.6774193548387096, 0.7419354838709677, nan]  \n",
      "0      [0.6774193548387096, 0.8076923076923077, 0.8]  \n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_size = train_dataset.tensors[0].shape[1]  # Get the number of features from your dataset\n",
    "output_size = 4   # 3 labels \n",
    "batch_size = 32\n",
    "epochs = 50  # Adjust based on your runtime requirement\n",
    "early_stopping_factor = 10\n",
    "clip_value = 1  # for gradient clipping\n",
    "\n",
    "# Hyperparameters to tune\n",
    "hidden_sizes = [input_size*4, input_size*5, input_size*6]\n",
    "learning_rates = [0.0001, 0.001, 0.01]\n",
    "weight_decays = [0, 1e-5, 1e-4]  # L2 regularization\n",
    "\n",
    "# Load validation data\n",
    "validation_data = pd.read_csv('DF_Radiomics_noduls_with_diagnose_test_data_scaled.csv')\n",
    "\n",
    "# Create the tensor dataset\n",
    "X_test = validation_data.drop(['Patient', 'Node', 'Labels'], axis=1).values\n",
    "y_test = validation_data['Labels'].values\n",
    "validation_dataset = TensorDataset(torch.tensor(X_test).float(), torch.tensor(y_test).float())\n",
    "\n",
    "# Use the function\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize DataFrame to store results\n",
    "results = pd.DataFrame(columns=[\"epoch\", \"epoch_loss\", \"best_loss\", \"hidden_size\", \"learning_rate\", \"weight_decay\", \"accuracy\", \"sensitivity\", \"specificity\"])\n",
    "\n",
    "# Grid search\n",
    "for hidden_size in hidden_sizes:\n",
    "    for learning_rate in learning_rates:\n",
    "        for weight_decay in weight_decays:\n",
    "            # Initialize model, loss function, and optimizer\n",
    "            model = FCNN(input_size, hidden_size, output_size).cuda()\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "            # DataLoader\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            \n",
    "                        # Initialize best loss to infinity for comparison in the first epoch\n",
    "            best_loss = float('inf')\n",
    "\n",
    "            # Patience counter\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Patience limit\n",
    "            patience_limit = 5\n",
    "            \n",
    "            # Training loop (as before)...\n",
    "            model.train()\n",
    "            for epoch in range(epochs):\n",
    "                epoch_loss = 0\n",
    "                for inputs, targets in train_loader:\n",
    "                \n",
    "                    # Move inputs and targets to the device\n",
    "                    inputs = inputs.to(device)\n",
    "                    targets = targets.to(device)\n",
    "                    \n",
    "                    targets = targets.long()\n",
    "\n",
    "                    # Zero the gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = model(inputs)\n",
    "\n",
    "                    # Compute loss\n",
    "                    loss = criterion(outputs, targets)\n",
    "\n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "\n",
    "                # Average epoch loss\n",
    "                epoch_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "                # If the training loss has improved, save the model and reset the patience counter\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    patience_counter = 0\n",
    "                    #torch.save(model.state_dict(), 'best_model.pth')\n",
    "                    #change to realtive path according to the hidden size, learning rate and weight decay\n",
    "                    torch.save(model.state_dict(), f'../models/best_model_{hidden_size}_{str(learning_rate).replace(\".\", \"_\")}_{weight_decay}.pth')\n",
    "\n",
    "                    \n",
    "                else:\n",
    "                    # If the training loss has not improved, increment the patience counter\n",
    "                    patience_counter += 1\n",
    "                    if patience_counter >= patience_limit:\n",
    "                        print(f\"Early stopping at epoch {epoch+1}/{epochs}, best loss: {best_loss}\")\n",
    "                        break\n",
    "\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss}\")\n",
    "\n",
    "            # Load the best model\n",
    "            model.load_state_dict(torch.load(f'../models/best_model_{hidden_size}_{str(learning_rate).replace(\".\", \"_\")}_{weight_decay}.pth'))\n",
    "\n",
    "            # Evaluate the model\n",
    "            accuracy, sensitivity, specificity = evaluate(model, validation_loader, device)\n",
    "\n",
    "            # Write results to DataFrame\n",
    "            # Check if results is a DataFrame with concat\n",
    "            results = pd.concat([results, pd.DataFrame([[epoch, epoch_loss, best_loss, hidden_size, learning_rate, weight_decay, accuracy, sensitivity, specificity]], columns=results.columns)])\n",
    "\n",
    "\n",
    "# Print the results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>epoch_loss</th>\n",
       "      <th>best_loss</th>\n",
       "      <th>hidden_size</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>sensitivity</th>\n",
       "      <th>specificity</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>0.105441</td>\n",
       "      <td>0.007929</td>\n",
       "      <td>690</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.870968</td>\n",
       "      <td>[0.85, 0.875, 0.8846153846153846]</td>\n",
       "      <td>[0.9302325581395349, 0.9555555555555556, 0.916...</td>\n",
       "      <td>6.283038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>40</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>460</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>[0.95, 0.875, 0.7692307692307693]</td>\n",
       "      <td>[0.972972972972973, 0.9574468085106383, 0.85]</td>\n",
       "      <td>6.229489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>0.032074</td>\n",
       "      <td>0.009340</td>\n",
       "      <td>690</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.854839</td>\n",
       "      <td>[0.9, 0.8125, 0.8461538461538461]</td>\n",
       "      <td>[0.9512195121951219, 0.9347826086956522, 0.891...</td>\n",
       "      <td>6.191387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29</td>\n",
       "      <td>0.074579</td>\n",
       "      <td>0.027501</td>\n",
       "      <td>460</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>[0.95, 0.8125, 0.7692307692307693]</td>\n",
       "      <td>[0.972972972972973, 0.9375, 0.8461538461538461]</td>\n",
       "      <td>6.127067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20</td>\n",
       "      <td>0.091267</td>\n",
       "      <td>0.034236</td>\n",
       "      <td>690</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>[0.95, 0.8125, 0.7692307692307693]</td>\n",
       "      <td>[0.9736842105263158, 0.9361702127659575, 0.846...</td>\n",
       "      <td>6.126449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>0.060604</td>\n",
       "      <td>0.060604</td>\n",
       "      <td>690</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.838710</td>\n",
       "      <td>[0.95, 0.8125, 0.7692307692307693]</td>\n",
       "      <td>[0.9743589743589743, 0.9347826086956522, 0.846...</td>\n",
       "      <td>6.125736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30</td>\n",
       "      <td>0.159257</td>\n",
       "      <td>0.006020</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>[0.95, 0.8125, 0.7307692307692307]</td>\n",
       "      <td>[0.972972972972973, 0.9361702127659575, 0.825]</td>\n",
       "      <td>6.049993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>0.205147</td>\n",
       "      <td>0.205147</td>\n",
       "      <td>460</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>[0.9, 0.8125, 0.7692307692307693]</td>\n",
       "      <td>[0.9473684210526315, 0.9375, 0.8421052631578947]</td>\n",
       "      <td>6.031285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42</td>\n",
       "      <td>0.172360</td>\n",
       "      <td>0.006611</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>[0.85, 0.8125, 0.8076923076923077]</td>\n",
       "      <td>[0.926829268292683, 0.9347826086956522, 0.8648...</td>\n",
       "      <td>6.019250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>0.065157</td>\n",
       "      <td>0.065157</td>\n",
       "      <td>690</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>[0.95, 0.8125, 0.6923076923076923]</td>\n",
       "      <td>[0.972972972972973, 0.9347826086956522, 0.8048...</td>\n",
       "      <td>5.973893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>0.225831</td>\n",
       "      <td>0.221439</td>\n",
       "      <td>460</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.806452</td>\n",
       "      <td>[0.9, 0.8125, 0.7307692307692307]</td>\n",
       "      <td>[0.9459459459459459, 0.9375, 0.8205128205128205]</td>\n",
       "      <td>5.953680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>0.110135</td>\n",
       "      <td>0.110135</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>[0.9, 0.8125, 0.6923076923076923]</td>\n",
       "      <td>[0.9459459459459459, 0.9361702127659575, 0.8]</td>\n",
       "      <td>5.877246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>0.068088</td>\n",
       "      <td>0.068088</td>\n",
       "      <td>690</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>[0.9, 0.8125, 0.6923076923076923]</td>\n",
       "      <td>[0.9459459459459459, 0.9361702127659575, 0.8]</td>\n",
       "      <td>5.877246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>0.030852</td>\n",
       "      <td>0.021105</td>\n",
       "      <td>460</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.790323</td>\n",
       "      <td>[0.8, 0.8125, 0.7692307692307693]</td>\n",
       "      <td>[0.9024390243902439, 0.9333333333333333, 0.842...</td>\n",
       "      <td>5.849931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22</td>\n",
       "      <td>0.221727</td>\n",
       "      <td>0.042762</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0010</td>\n",
       "      <td>0</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>[0.95, 0.875, 0.5384615384615384]</td>\n",
       "      <td>[0.96875, 0.9565217391304348, 0.7391304347826086]</td>\n",
       "      <td>5.785928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>0.269517</td>\n",
       "      <td>0.231874</td>\n",
       "      <td>460</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>[0.95, 0.8125, 0.5769230769230769]</td>\n",
       "      <td>[0.9705882352941176, 0.9347826086956522, 0.75]</td>\n",
       "      <td>5.752858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>49</td>\n",
       "      <td>0.152644</td>\n",
       "      <td>0.152644</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>[0.85, 0.8125, 0.6538461538461539]</td>\n",
       "      <td>[0.918918918918919, 0.9361702127659575, 0.775]</td>\n",
       "      <td>5.704500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43</td>\n",
       "      <td>0.271777</td>\n",
       "      <td>0.267128</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.725806</td>\n",
       "      <td>[0.9, 0.75, 0.5769230769230769]</td>\n",
       "      <td>[0.9375, 0.9166666666666666, 0.75]</td>\n",
       "      <td>5.556896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>0.934821</td>\n",
       "      <td>0.777771</td>\n",
       "      <td>460</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.677419</td>\n",
       "      <td>[0.4, 0.6875, 0.8846153846153846]</td>\n",
       "      <td>[0.7692307692307693, 0.8888888888888888, 0.888...</td>\n",
       "      <td>5.196543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>0.917315</td>\n",
       "      <td>0.829969</td>\n",
       "      <td>460</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.596774</td>\n",
       "      <td>[0.75, 0.625, 0.46153846153846156]</td>\n",
       "      <td>[0.8333333333333334, 0.8823529411764706, 0.674...</td>\n",
       "      <td>4.823418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>1.090012</td>\n",
       "      <td>0.826014</td>\n",
       "      <td>460</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.580645</td>\n",
       "      <td>[0.8, 0.4375, 0.5]</td>\n",
       "      <td>[0.8620689655172413, 0.8235294117647058, 0.704...</td>\n",
       "      <td>4.708289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>1.149465</td>\n",
       "      <td>0.937690</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.516129</td>\n",
       "      <td>[0.35, 0.8125, 0.46153846153846156]</td>\n",
       "      <td>[0.75, 0.9, 0.6666666666666666]</td>\n",
       "      <td>4.456834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16</td>\n",
       "      <td>1.170959</td>\n",
       "      <td>0.883630</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>[0.15, 0.6875, 0.6153846153846154]</td>\n",
       "      <td>[0.7068965517241379, 0.8529411764705882, 0.6875]</td>\n",
       "      <td>4.184093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12</td>\n",
       "      <td>1.066082</td>\n",
       "      <td>0.988284</td>\n",
       "      <td>690</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.483871</td>\n",
       "      <td>[0.0, 0.375, 0.9230769230769231]</td>\n",
       "      <td>[0.6774193548387096, 0.8076923076923077, 0.8]</td>\n",
       "      <td>4.067060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>1.035649</td>\n",
       "      <td>1.001519</td>\n",
       "      <td>575</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.467742</td>\n",
       "      <td>[0.25, 0.0, 0.9230769230769231]</td>\n",
       "      <td>[0.7169811320754716, 0.7419354838709677, 0.777...</td>\n",
       "      <td>3.877513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18</td>\n",
       "      <td>1.072342</td>\n",
       "      <td>1.039863</td>\n",
       "      <td>690</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.6774193548387096, 0.7419354838709677, nan]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>1.116280</td>\n",
       "      <td>1.074529</td>\n",
       "      <td>690</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.419355</td>\n",
       "      <td>[0.0, 0.0, 1.0]</td>\n",
       "      <td>[0.6774193548387096, 0.7419354838709677, nan]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  epoch  epoch_loss  best_loss hidden_size  learning_rate weight_decay  \\\n",
       "0    28    0.105441   0.007929         690         0.0010            0   \n",
       "0    40    0.001394   0.000838         460         0.0010      0.00001   \n",
       "0    26    0.032074   0.009340         690         0.0010      0.00001   \n",
       "0    29    0.074579   0.027501         460         0.0010            0   \n",
       "0    20    0.091267   0.034236         690         0.0010       0.0001   \n",
       "0    49    0.060604   0.060604         690         0.0001       0.0001   \n",
       "0    30    0.159257   0.006020         575         0.0010       0.0001   \n",
       "0    49    0.205147   0.205147         460         0.0001            0   \n",
       "0    42    0.172360   0.006611         575         0.0010      0.00001   \n",
       "0    49    0.065157   0.065157         690         0.0001            0   \n",
       "0    49    0.225831   0.221439         460         0.0001       0.0001   \n",
       "0    49    0.110135   0.110135         575         0.0001            0   \n",
       "0    49    0.068088   0.068088         690         0.0001      0.00001   \n",
       "0    26    0.030852   0.021105         460         0.0010       0.0001   \n",
       "0    22    0.221727   0.042762         575         0.0010            0   \n",
       "0    49    0.269517   0.231874         460         0.0001      0.00001   \n",
       "0    49    0.152644   0.152644         575         0.0001      0.00001   \n",
       "0    43    0.271777   0.267128         575         0.0001       0.0001   \n",
       "0    18    0.934821   0.777771         460         0.0100      0.00001   \n",
       "0    15    0.917315   0.829969         460         0.0100            0   \n",
       "0    15    1.090012   0.826014         460         0.0100       0.0001   \n",
       "0    14    1.149465   0.937690         575         0.0100       0.0001   \n",
       "0    16    1.170959   0.883630         575         0.0100      0.00001   \n",
       "0    12    1.066082   0.988284         690         0.0100       0.0001   \n",
       "0    10    1.035649   1.001519         575         0.0100            0   \n",
       "0    18    1.072342   1.039863         690         0.0100            0   \n",
       "0    14    1.116280   1.074529         690         0.0100      0.00001   \n",
       "\n",
       "   accuracy                          sensitivity  \\\n",
       "0  0.870968    [0.85, 0.875, 0.8846153846153846]   \n",
       "0  0.854839    [0.95, 0.875, 0.7692307692307693]   \n",
       "0  0.854839    [0.9, 0.8125, 0.8461538461538461]   \n",
       "0  0.838710   [0.95, 0.8125, 0.7692307692307693]   \n",
       "0  0.838710   [0.95, 0.8125, 0.7692307692307693]   \n",
       "0  0.838710   [0.95, 0.8125, 0.7692307692307693]   \n",
       "0  0.822581   [0.95, 0.8125, 0.7307692307692307]   \n",
       "0  0.822581    [0.9, 0.8125, 0.7692307692307693]   \n",
       "0  0.822581   [0.85, 0.8125, 0.8076923076923077]   \n",
       "0  0.806452   [0.95, 0.8125, 0.6923076923076923]   \n",
       "0  0.806452    [0.9, 0.8125, 0.7307692307692307]   \n",
       "0  0.790323    [0.9, 0.8125, 0.6923076923076923]   \n",
       "0  0.790323    [0.9, 0.8125, 0.6923076923076923]   \n",
       "0  0.790323    [0.8, 0.8125, 0.7692307692307693]   \n",
       "0  0.758065    [0.95, 0.875, 0.5384615384615384]   \n",
       "0  0.758065   [0.95, 0.8125, 0.5769230769230769]   \n",
       "0  0.758065   [0.85, 0.8125, 0.6538461538461539]   \n",
       "0  0.725806      [0.9, 0.75, 0.5769230769230769]   \n",
       "0  0.677419    [0.4, 0.6875, 0.8846153846153846]   \n",
       "0  0.596774   [0.75, 0.625, 0.46153846153846156]   \n",
       "0  0.580645                   [0.8, 0.4375, 0.5]   \n",
       "0  0.516129  [0.35, 0.8125, 0.46153846153846156]   \n",
       "0  0.483871   [0.15, 0.6875, 0.6153846153846154]   \n",
       "0  0.483871     [0.0, 0.375, 0.9230769230769231]   \n",
       "0  0.467742      [0.25, 0.0, 0.9230769230769231]   \n",
       "0  0.419355                      [0.0, 0.0, 1.0]   \n",
       "0  0.419355                      [0.0, 0.0, 1.0]   \n",
       "\n",
       "                                         specificity     score  \n",
       "0  [0.9302325581395349, 0.9555555555555556, 0.916...  6.283038  \n",
       "0      [0.972972972972973, 0.9574468085106383, 0.85]  6.229489  \n",
       "0  [0.9512195121951219, 0.9347826086956522, 0.891...  6.191387  \n",
       "0    [0.972972972972973, 0.9375, 0.8461538461538461]  6.127067  \n",
       "0  [0.9736842105263158, 0.9361702127659575, 0.846...  6.126449  \n",
       "0  [0.9743589743589743, 0.9347826086956522, 0.846...  6.125736  \n",
       "0     [0.972972972972973, 0.9361702127659575, 0.825]  6.049993  \n",
       "0   [0.9473684210526315, 0.9375, 0.8421052631578947]  6.031285  \n",
       "0  [0.926829268292683, 0.9347826086956522, 0.8648...  6.019250  \n",
       "0  [0.972972972972973, 0.9347826086956522, 0.8048...  5.973893  \n",
       "0   [0.9459459459459459, 0.9375, 0.8205128205128205]  5.953680  \n",
       "0      [0.9459459459459459, 0.9361702127659575, 0.8]  5.877246  \n",
       "0      [0.9459459459459459, 0.9361702127659575, 0.8]  5.877246  \n",
       "0  [0.9024390243902439, 0.9333333333333333, 0.842...  5.849931  \n",
       "0  [0.96875, 0.9565217391304348, 0.7391304347826086]  5.785928  \n",
       "0     [0.9705882352941176, 0.9347826086956522, 0.75]  5.752858  \n",
       "0     [0.918918918918919, 0.9361702127659575, 0.775]  5.704500  \n",
       "0                 [0.9375, 0.9166666666666666, 0.75]  5.556896  \n",
       "0  [0.7692307692307693, 0.8888888888888888, 0.888...  5.196543  \n",
       "0  [0.8333333333333334, 0.8823529411764706, 0.674...  4.823418  \n",
       "0  [0.8620689655172413, 0.8235294117647058, 0.704...  4.708289  \n",
       "0                    [0.75, 0.9, 0.6666666666666666]  4.456834  \n",
       "0   [0.7068965517241379, 0.8529411764705882, 0.6875]  4.184093  \n",
       "0      [0.6774193548387096, 0.8076923076923077, 0.8]  4.067060  \n",
       "0  [0.7169811320754716, 0.7419354838709677, 0.777...  3.877513  \n",
       "0      [0.6774193548387096, 0.7419354838709677, nan]       NaN  \n",
       "0      [0.6774193548387096, 0.7419354838709677, nan]       NaN  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate a score from accuracy, sensitivity and specificity wher sensitivity and specificity are a list of 3 values\n",
    "results['score'] = results['accuracy'] + results['sensitivity'].apply(lambda x: sum(x)) + results['specificity'].apply(lambda x: sum(x))\n",
    "#sort by score\n",
    "results = results.sort_values(by=['score'], ascending=False)\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
